\documentclass[a4paper, 12pt]{article}

\usepackage{geometry}

\usepackage{cmap}						% Улучшенный поиск русских слов в полученном pdf-файле
\usepackage[T2A]{fontenc}				% Поддержка русских букв
\usepackage[utf8]{inputenc}				% Кодировка utf8
\usepackage[english, russian]{babel}	% Языки: русский, английский
\usepackage[unicode]{hyperref}			% Русский язык для оглавления pdf
\usepackage{bookmark}					% Оглавление в pdf

\usepackage{titlesec}					% Форматирование заголовков
\usepackage{indentfirst} 				% Красная строка
\usepackage{graphicx} 					% Подключаем пакет работы с графикой
\usepackage{amssymb,amsmath,amsthm}
\usepackage{algorithm,algpseudocode}


\usepackage{subcaption}					% Для названий вложенных изображений

\graphicspath{{../images/}} 			% Пути к изображениям

\geometry{a4paper,top=2cm,bottom=2cm,left=2.5cm,right=1cm}	% Геомтерия страницы
\makeatletter
%	\bibliographystyle{naturemag}
	\bibliographystyle{gost2008p}
	\renewcommand{\@biblabel}[1]{#1.}	% Заменяем библиографию с квадратных скобок на точку:
\makeatother

\titleformat{\section}
	{\normalfont\large\centering}{\thesection.}{5pt}{\large\MakeUppercase}
\titleformat{\subsection}
{\itshape\centering}{\thesubsection.}{5pt}{\itshape}

%\renewcommand{\baselinestretch}{2.0}	% Двойной интервал

% Настройка теоремоподобных окружений
\theoremstyle{plain}
\newtheorem{Theorem}{Теорема}
\newtheorem{Lemma}[Theorem]{Лемма}
\newtheorem{Pred}{Утверждение}
\newtheorem{Corollary}{Следствие}
\newtheorem{Def}{Определение}
\newenvironment{Proof}%
	{\par\noindent{\bf Доказательство.}}%
	{\hfill$\scriptstyle\blacksquare$}
\floatname{algorithm}{}

\renewcommand\labelenumi{\theenumi )}	% Нумерованный перечень со скобками

\begin{document}
	\title{\MakeUppercase{Распознавание образов как функциональная составляющая элементов картины мира}}
	\author{А.\,И. Панов\\ Институт системного анализа РАН\\ 117213, Россия, Москва, пр. 60"--~летия Октября, 9\\ e-mail: pan@isa.ru}
	\date{}
	
	\maketitle

	\begin{abstract}
		Рассматривается алгоритм работы образной компоненты элементарной единицы индивидуального знаний субъекта деятельности. В основе работы алгоритма лежат нейрофизиологические данные. Формулируется ряд статических и динамических постановок задач распознавания (классификации) в русле алгебраического подхода. Доказываются теоремы корректности линейных замыканий образуемых операторов распознавания, результаты которых интерпретируются как возможность провести обучение в картине мира субъекта таким образом, что он будет правильно категоризовать поступающий стимул.
	\end{abstract}

	\section*{Введение}
	Исследования психологов и нейрофизиологов свидетельствуют о~существовании элементарных единиц индивидуального знания, которые, с~одной стороны, обладают независимой от~области и типа деятельности субъекта универсальной структурой, а с~другой стороны, обладают одинаковыми для всех таких элементов функциями при формировании человеческого поведения. Существуют разные точки зрения при описании таких единиц, обеспечивающих элементарные когнитивные функции субъекта: группы нейронов \cite{Edelmen1981}, фокус взаимодействия нейронных ансамблей \cite{IvanitskyE1996}, локальный анализатор признаков \cite{Vartanov2011}, нейроны"--~символы \cite{Chernavsky2012}, объемлющие характеристики \cite{Sergin2009} и др. Для описания элементарных единиц индивидуального знания в данной работе используется понятие \textit{знака}, рассматриваемое как в~логических работах \cite{Frege2000,Pirs2000}, так и в~психологических исследованиях по теории деятельности \cite{Leontiev1975}.

	В теории деятельности Леонтьева знак представляет некоторый процесс или объект внешней среды и имеет три компоненты, определяющие его роль в деятельности субъекта: образ, значение и личностный смысл. Образ знака представляет собой процедуры отделения представляемого объекта от других объектов и одновременно процедуры построения внутреннего описания этого объекта. Значение знака определяет общепринятые в культурно"--~исторической среде, к которой принадлежит субъект, действия, совершаемые над этим объектом или с ним. Личностные смыслы знака определяют действия, совершаемые с этим объектом при учёте внутренних характеристик субъекта (его потребностей и возможностей) (Рисунок \ref{fig:sign}).
	
	Для~построения модели знака был введён четвёртый компонент "--- имя (\cite{Pospelov2002, PanovA2014a}). Такое представление о структуре знака позволило описать процедуры формирования знака, самоорганизации на множестве знаков и построить модель картины мира субъекта. Однако строение самих компонентов знака и их связь с нейрофизиологическими данными о строении и механизмах работы мозга оставались за рамками модели. В~настоящем исследовании рассмотрено описание образной компоненты, алгоритм работы которой с~одной стороны основан на одной из нейрофизиологических моделей, а с~другой "--- реализует основные психологические функции этой компоненты.
	
	В~качестве базисной нейрофизиологической модели была выбрана модель неокортекса человека, описываемая в работах \cite{Hawkins2009,George20051812}. В~предлагаемой авторами так называемой иерархической временной памяти (ИВП"--~модель) основным элементом является регион неокортекса, состоящий из шестислойных колонок нейронов. Регионы организованы в иерархию (несколько иерархий с учётом модальности поступающих сигналов), в которой имеются как восходящие потоки информации (о распознаваемых объектах), так и нисходящие "--- управляющие процессом распознавания. В~процессе обучения данные сохраняются в~виде последовательности связанных колонок региона, именно такая последовательность и хранит в себе развёрнутое во времени внутреннее представление об объекте.
	
	В качестве модели неокортекса в данной статье рассмотрен объект под названием \textit{распознающий блок}, который реализует основные принципы работы ИВП"--~модели: иерархичность представления, предсказывающее воздействие верхних уровней иерархии, хранение развёрнутого во времени представления в виде последовательности признаков. При этом мы будем считать, что один из распознаваемых объектов представляется некоторым знаком, образом которого и будет являться та последовательность признаков, которая хранится в соответствующем распознающем блоке (Рисунок \ref{fig:sign_rb}). В~следующих параграфах статьи изложено формальное описание распознающего блока и исследованы его свойства в контексте задачи на распознавание.
	\begin{figure}
		\centering
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\linewidth]{sign}
			\caption{компоненты знака}
			\label{fig:sign}
		\end{subfigure}
		\qquad
		\begin{subfigure}[b]{0.5\textwidth}
			\includegraphics[width=\linewidth]{sign_rb}
			\caption{образная компонента знака}
			\label{fig:sign_rb}
		\end{subfigure}		
		\caption{Структура знака.}
	\end{figure}
		
	\section{Распознающий блок}
	Будем рассматривать объект $R_i^j$, который будем называть \textit{распознающим блоком} уровня $j$ с индексом $i$ или просто распознающим блоком. Опишем кратко функции введённого объекта, а затем определим алгоритм его работы формально.  Далее будем пользоваться понятием \textit{признака}, который будем понимать как составную часть информационного, описательного представления некоторой сущности, явления или процесса.
	
	Каждый распознающий блок, исходя из своего названия, распознает, или, применительно к~низкоуровневым сигналам, измеряет, некоторые признаки.  Распознавание (измерение) заключается в сопоставлении признака "--- весовому значению, характеризующему тот факт, удаётся ли собрать (измерить) признак из составляющих его низкоуровневых входных признаков, информация о которых содержится во входном векторе. Такой вес будем называть \textit{весом присутствия признака} во входном векторе.
	
	Входной вектор, в свою очередь, представляет собой весовой вектор присутствия низкоуровневых признаков, по которым распознаются выходные признаки. Распознающий блок обладает состоянием, которое представляет собой также весовой вектор присутствия входных признаков, но в следующий момент времени. Такой вектор будем называть \textit{вектором ожиданий}. Запишем все вышесказанное строго.
	
	Пусть заданы множества $\{R_i^j\}$ и $\{f_k\}$. Множество $\{R_i^j\}$ будем называть совокупностью распознающих блоков, а множество $\{f_k\}$ "--- совокупностью допустимых признаков. Введём бинарное отношение $\dashv$, определённое на паре множеств $\{f_k\}$ и $\{R_i^j\}$, и будем читать $f_k{\dashv}R_i^j$ как <<признак $f_k$ распознаётся блоком $R_i^j$>> или как <<признак $f_k$ распознаётся блоком $R_i^j$>>. Множество всех распознаваемых блоком $R_i^j$ признаков будем обозначать $F_i^{*j}$, т.е. ${\forall}f^*{\in}F_i^{*j} f^*{\dashv}R_i^j, F_i^{*j}{\subseteq}\{f_k\}$.
	
	Рассмотрим связный ориентированный (ярусный) граф $G_R=(V,E)$, где $V$ - множество вершин, $E$ - множество рёбер. Каждую вершину $v$, принадлежащую $j$"~ому ярусу графа $G_R$, будем связывать с соответствующим распознающим блоком $R_i^j$ уровня $j$, а ребро $e=(v,u){\in}E$ будем интерпретировать как иерархическую связь между соответствующим вершине $v$ дочерним блоком $R_{i_1}^{j_1 }$ и соответствующим вершине $u$ блоком"--~родителем $R_{i_2}^{j_2}$.
	
	Рассмотрим распознающий блок $R_i^j$. Определим множество $F_i^j{\subseteq}{f_k}$ таких признаков, что для любого $f{\in}F_i^j$ существует распознающий блок $R_k^{j-1}$ уровня $j-1$, дочерний по отношению к блоку $R_i^j$, такой, что $f{\dashv}R_k^{j-1}$. Такое множество $F_i^j$ будем называть совокупностью входных признаков распознающего блока $R_i^j$. Для каждого признака $f^*{\in}F_i^{*j}$ введём функцию распознавания $\hat{f}(x_1,\dots,x_q )=x^*$, где $x^*{\in}[0,1]$ "--- весовой вектор присутствия распознаваемого признака $f^*$, а $x_1,\dots,x_q{\in}[0,1]$ "--- весовой вектор присутствия признаков из множества $F_i^j$. Множество таких функций для распознающего блока $R_i^j$ обозначим как $\hat{F}_i^j$.
	
	Пусть мощность множества распознаваемых признаков $F_i^{*j}$ и множества функций распознавания $\hat{F}_i^j$ равна $l_i^j$, а мощность множества входных признаков $F_i^j$ равна $q_i^j$. Введём упорядоченное множество локальных моментов времени $T_i^j$ для распознающего блока $R_i^j$. Для каждого распознающего блока определим характерный масштаб времени $h_i^j$, за который происходит один цикл вычисления в распознающем блоке $R_i^j$. 
	
	В начале $s$-ого цикла вычисления (момент времени $\tau_s\in{T_i^j}$)  распознающий блок $R_i^j$ получает на вход вектор длины $_i^jl$ ожиданий $\hat{x}_i^{j+1}(\tau_s)$, вычисляемый по формуле среднего от векторов ожиданий, получаемых от родительских относительно блока $R_i^j$ распознающих блоков $R_k^{j+1}$:
	\[
	\hat{x}_i^{j+1}(\tau_s)=\frac{1}{N_i^j}\sum_{k{\in}K_i^{j+1}}\hat{x}_k^{j+1}(\tau_s),
	\]
	где $N_i^j$ - количество родительских блоков, $K_i^{j+1}$ - множество индексов родительских относительно $R_i^j$ распознающих блоков. Далее в~каждый момент времени $t\in{T_i^j}$, $\tau_s\leqslant{t}\leqslant\tau_s+h_i^j$,  распознающий блок $R_i^j$ получает на~вход весовой вектор $\bar{x}_i^j(t)$ длины $l_i^j$ присутствия входных признаков из~множества $F_i^j$, вычисляет выходной весовой вектор $\bar{x}_i^{*j}(t)$ длины $l_i^j$ присутствия распознаваемых признаков из~множества $F_i^{*j}$, вычисляет вектор длины $q_i^j$ ожиданий $\hat{x}_i^j(t)$ присутствия входных признаков в следующий момент времени (Рисунок~\ref{fig:rb_cycle}).
		
	\begin{figure}[h]
		\centering
		\includegraphics[width=1.0\linewidth]{rb_cycle}
		\caption{Вычислительные циклы распознающего блока. Символом <<И>> обозначен этап инициализации в начале каждого цикла.}
		\label{fig:rb_cycle}  
	\end{figure}
	
	\section{Алгоритм работы распознающего блока}
	Опишем распознающий блок $R_i^j$ с~точки зрения классической теории динамических систем \cite{Kalman1971}. Обозначим множество возможных мгновенных значений выходных векторов распознающего блока $R_i^j$ как $X_i^{*j}$. Очевидно, что $X_i^{*j}$ является векторным пространством. Обозначим множество возможных мгновенных значений весового вектора присутствия входных признаков как $X_i^j$. Очевидно, что $X_i^j$ также является векторным пространством. Определим \textit{входное воздействие} $\omega_i^j:T{\to}X_i^j$ и \textit{выходную величину} $\gamma_i^j:T{\to}X_i^{*j}$ в~смысле теории динамических систем. Будем считать, что совокупность всех возможных мгновенных значений векторов ожиданий образует \textit{множество состояний} $\hat{X}_i^j$ распознающего блока $R_i^j$. Определим \textit{функцию переходов} $\varphi_i^j(t;\tau_s,\hat{x}_i^{j+1},\omega_i^j)=\hat{x}_i^j$ в~смысле теории динамических систем. Множество $\hat{X}_i^j$ в таком случае интерпретируется как \textit{множество состояний} распознающего блока $R_i^j$. Также зададим \textit{выходное отображение} $\eta_i^j:T{\times}\hat{X}_i^j{\to}X_i^{*j}$ в~смысле теории динамических систем, определяющее выходные векторы $\bar{x}_i^{*j}(t)=\eta_i^j(t,\hat{x}_i^j(t))$ (Рисунок~\ref{fig:rb_io}).
	
	\begin{figure}[h]
		\includegraphics[width=\linewidth]{rb_io}
		\caption{Схема входных и выходных отображений распознающего блока.}
		\label{fig:rb_io}
	\end{figure}
	
	Будем рассматривать распознающий блок $R_i^j$ как динамическую систему с дискретным временем, т.~е. считать множество моментов времени $T_i^j$ множеством целых чисел. Каждой функции распознавания $\hat{f}_k$ из множества $\hat{F}_i^j$ будем ставить в соответствие набор \textit{матриц предсказания} $Z_k=\{Z_1^k,…,Z_m^k\}$ размерности $q_i^j\times h_i^j$, где $h_i^j$ "--- характерное время распознающего блока $R_i^j$. Столбец $\bar{z}_u^r=(z_{u1}^k,…,z_{uq}^k)$ матрицы $Z_r^k$ интерпретируется как вектор предсказания присутствия входных признаков из множества $F_i^j$ в момент времени $\tau_s+u$, при этом $z_{uv}^k\in\{0,1\}$, т.е. вектор $\bar{z}_u^r$ является булевым вектором. Сама матрица $Z_r^k$ задаёт, таким образом, последовательность событий, наличие которых свидетельствует о~присутствии распознаваемого функцией $\hat{f}_k$ признака. Множество всех матриц предсказания распознающего блока $R_i^j$ будем обозначать как $\mathcal{Z}_i^j$.
	
	В листингах \ref{alg:th_init} и \ref{alg:th_cycle} приведён алгоритм $\mathfrak{A}_{th}$ вычислительного цикла распознающего блока, в котором рассчитываются значения функции переходов $\varphi_i^j(\tau_s+t;\tau_s,\hat{x}_i^{j+1},\omega_i^j)$, $1\leqslant{t}\leqslant h_i^j-1$, и выходного отображения $\eta_i^j(\tau_s+t,\hat{x}_i^j(\tau_s+t))$, $1\leqslant{t}\leqslant h_i^j-1$. В листингах используется функция $W$ нормировки весовых функций, действие которой для вектора $\bar x=(x_1,\dots,x_n)$ описывается формулой
	\[
		W(\bar x)=\left(\frac{x_1}{\max\limits_i x_i},\dots,\frac{x_n}{\max\limits_i x_i}\right).
	\] 
	Кратко опишем шаги алгоритма.
	
	\begin{algorithm}[H]
		\caption{Алгоритм $\mathfrak{A}_{th}$ (часть I, инициализация)}\label{alg:th_init}
		\begin{algorithmic}[1]
			\Require $\tau_s, \hat{x}_i^{j+1}(\tau_s), \omega_i^j$;
			\Ensure $\varphi_i^j, \eta_i^j$;
			
			\State $\hat{F}^*=\varnothing,Z^*=\varnothing,t=0$;
			\State $c_1\in(0,1), c_2\in(0,1)$;
			
			\ForAll{компонент $\hat{x}_{ik}^{j+1}$ вектора $\hat{x}_i^{j+1}(\tau_s)=(\hat{x}_{i1}^{j+1},\hat{x}_{i2}^{j+1},\dots,\hat{x}_{il}^{j+1})$} \label{alst:init_start}
				\If{$\hat{x}_{ik}^{j+1}{\ge}c_1$} \label{alst:select_f}
					\State $\hat{F}^*:=\hat{F}^*\cup\{\hat{f}_k\}$;
				\EndIf
			\EndFor
			
			\State $\bar x_i^j:=\omega_i^j(\tau_s)$;
			
			\ForAll{функций распознавания $\hat{f}_k\in\hat{F}^*$}
				\ForAll{$Z_r^k\in\mathcal{Z}_k$, соответствующих функции распознавания $\hat{f}_k$,}
					\If{$\frac{\|\bar{z}_1^r-\bar{x}_i^j\|}{\|\bar{z}_1^r\|+\|\bar{x}_i^j\|}<c_2$} \label{alst:select_z}
						\State $Z^*:=Z^*\cup\{Z_r^k\}$;
					\EndIf
				\EndFor
			\EndFor
			
			\State $\bar N:=(|\{Z_r^1|Z_r^1\in Z^*\}|,\dots,|\{Z_r^{l_i^j}|Z_r^{l_i^j}\in Z^*\}|)$; \label{alst:init_calc_out1}
			\State $\bar{x}_i^{*j}:=W(\bar N)$; \label{init_alst:calc_out2}
			\State $\eta(\tau_s, \hat{x}_i^j(\tau_s))=\bar{x}_i^{*j}$; \label{alst:init_calc_out3}
			
			\State $\hat x_i^j=W(\sum_{\hat f_k\in\hat F^*}\hat x_{ik}^{j+1}\sum_{Z_r^k\in Z^*}\bar z_2^r)$; \label{alst:init_state}
			\State $\varphi(\tau_s+1;\tau_s,\hat{x}_i^{j+1}, \omega)=\hat{x}_i^j(\tau_s+1)=\hat{x}_i^j$;\label{alst:init_end}			
			\algstore{ru_alg}
		\end{algorithmic}
	\end{algorithm}
	
	Вычислительный цикл распознающего блока начинается с инициализации состояния при~помощи управляющего воздействия от верхних уровней иерархии "--- вектора ожиданий $\hat x_i^{j+1}(\tau_s)$ (шаги \ref{alst:init_start}--\ref{alst:init_end}). Начальное состояние определяется отбором тех распознаваемых признаков из множества $F_i^{*j}$, которые предсказываются на основе состояния блоков верхнего уровня. Первая константа $c_1$ определяет порог предсказываемого веса присутствия распознаваемых признаков, вышей которого соответствующие функции распознавания попадают во множество активных функций $\hat F^*$ (шаг \ref{alst:select_f}). Далее производится отбор тех матриц предсказания активных функций распознавания, для которых обычное расстояние по норме $\|x\|=\sum_i |x_i|$ первого столбца $\bar z_1^r$ от входного вектора $\bar z_i^j$ в начальный момент времени не превышает второй константы $c_2$ (шаг \ref{alst:select_z}). На основе активных матриц предсказания методом голосования вычисляется выходной вектор в начальный момент времени $\bar x_i^{j*}(\tau_s)$ (шаги \ref{alst:init_calc_out1}--\ref{alst:init_calc_out3}).
		
	Начальное состояние $\hat x_i^j(\tau_s+1)$ определяется как нормированный вектор, $s$-ый компонент которого равен сумме всех $s$-ых элементов вторых колонок активных матриц предсказания с весами, соответствующими элементам вектора ожиданий $\hat x_i^{j+1}(\tau_s)$ (шаг \ref{alst:init_state}). Т.~к. используется представление о будущем входном сигнале (вторая колонка матриц предсказания), то $\hat x_i^j(\tau_s+1)$ играет роль предсказывающего управляющего вектора для нижних уровней иерархии.
	
	После инициализации состояния начинает выполняться тело основного цикла, в котором до тех пор, пока время не превысит характерное время распознающего блока $h_i^j$ повторяется вычисление выходного вектора и состояния в следующий момент времени (шаги \ref{alst:cycle_start}--\ref{alst:cycle_end}). В начале обновляется множество активных матриц предсказания $Z^*$ за счёт удаления тех матриц, соответствующие столбцы которых достаточно сильно отличаются от текущего входного вектора $\bar x_i^j$ (шаг \ref{alst:update_z}). Далее методом голосования по количеству матриц в множестве активных матриц предсказания, отвечающих за соответствующий выходной признак, вычисляется выходной вектор $\bar x_i^{j*}$ (шаги \ref{alst:calc_out1}--\ref{alst:calc_out3}).
			
	В завершение тела основного цикла вычисляется состояние в следующий момент времени $\hat x_i^j(\tau_s+t+1)$. Как и на этапе инициализации, вектор ожиданий равен нормированному вектору, элементы которого равны сумме элементов столбцов всех активных матриц предсказания, соответствующих текущему моменту времени с учётом весов начального управляющего вектора $\hat x_i^{j+1}(\tau_s)$ (шаги \ref{alst:calc_state1}--\ref{alst:calc_state2}).
	\begin{algorithm}[H]
		\caption{Алгоритм $\mathfrak{A}_{th}$ (часть II, основной цикл)}\label{alg:th_cycle}
		\begin{algorithmic}[1]
			\algrestore{ru_alg}
			\State $t=1$;
			\While{$t\leqslant{h_i^j}-1$} \label{alst:cycle_start}
				\State $\bar{x}_i^j:=\omega(\tau_s+t)$;

				\ForAll{матриц предсказания $Z_r^k$ из множества $Z^*$}
					\If{$\frac{\|\bar{z}_{t+1}^r-\bar{x}_i^j\|}{\|\bar{z}_{t+1}^r\|+\|\bar{x}_i^j\|}\geqslant{c_2}$} \label{alst:update_z}
						\State $Z^*:=Z^*\setminus\{Z_r^k\}$;
					\EndIf
				\EndFor

				\State $\bar N=(|\{Z_r^1|Z_r^1\in Z^*\}|,\dots,|\{Z_r^{l_i^j}|Z_r^{l_i^j}\in Z^*\}|)$; \label{alst:calc_out1}
				\State $\bar{x}_i^{*j}:=W(\bar N)$; \label{alst:calc_out2}
				\State $\eta(\tau_s+t, \hat{x}_i^j(\tau_s+t))=\bar{x}_i^{*j}$; \label{alst:calc_out3}
							
				\State $t=t+1$;
				\If{$t\leqslant{h}_i^j-2$}
					\State $\hat{x}_i^j:=W(\sum_{\hat f_k\in\hat F^*}\hat x_{ik}^{j+1}\sum_{Z_r^k\in Z^*}\bar z_t^r)$; \label{alst:calc_state1}
								
					\State $\varphi(\tau_s+t;\tau_s,\hat{x}_i^{j+1}, \omega)=\hat{x}_i^j(\tau_s+t)=\hat{x}_i^j$; \label{alst:calc_state2}
				\EndIf
			\EndWhile \label{alst:cycle_end}
		\end{algorithmic}
	\end{algorithm}
	
	\section{Статическая постановка задачи классификации}
	\subsection{Начальный момент времени}
	В начале рассмотрим статический случай, т.~е. зафиксируем момент времени $t$, равный началу некоторого $s$-го вычислительного цикла $\tau_s$. В этом случае, распознающий блок $R_i^j$ можно рассматривать как \textit{статический оператор распознавания} $R_i^j(\hat{x}_i^{j+1},\mathcal{Z}_i^j,\bar{x}_i^j)=\bar{x}_i^{*j}$. Напомним, что $\bar{x}_i^{*j}$ "--- это весовой вектор присутствия распознаваемых признаков $f_1^*,\dots,f_l^*$ из множества $F_i^{*j}$. Далее кратко будем записывать $R(\hat{x},\mathcal{Z},\bar{x})=\bar{x}^*$ и везде, где это возможно, будем опускать индексы $j$ и $i$.
	
	Введём совокупность задач $\{Q\}$ аналогично работе \cite{Zhuravlev1977}. Задача $Q(\hat{x},\bar{x},\alpha_1,\dots,\alpha_l)\in\{Q\}$ состоит в построении оператора, вычисляющего по поступившему вектору ожиданий $\hat{x}$ и входному вектору $\bar{x}$ значения $\alpha_1,\dots,\alpha_l\in\{0,1\}$ присутствия признаков $f_1^*,\dots,f_l^*$. Другими словами, искомый алгоритм $\mathcal{A}^*$ переводит набор $(\hat{x},\bar{x})$ в вектор $\bar{\alpha}=(\alpha_1,\dots,\alpha_l)$, который будем называть \textit{информационным вектором} входного вектора $\bar{x}$ (Рисунок \ref{fig:rb_correct_stat0}).
	
	\begin{figure}[h]
		\centering
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\linewidth,page=1]{rb_correct}
			\caption{для момента времени $\tau_s$}
			\label{fig:rb_correct_stat0}
		\end{subfigure}
		\qquad
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=0.9\linewidth,page=2]{rb_correct}
			\caption{для момента времени $t\tau_s<t\leqslant\tau_s+h_i^j$}
			\label{fig:rb_correct_statt}
		\end{subfigure}		
		\caption{Статические схемы корректности.}
	\end{figure}
	
	Пусть множество $\{\mathcal{A}\}$ состоит из алгоритмов, переводящих пары $(\hat{x},\bar{x})$ в векторы $\bar{\beta}$, составленные из элементов $0,1,\Delta:\mathcal{A}(\hat{x},\bar{x})=\bar{\beta}$. Если $\beta_i\in\{0,1\}$, то $\beta_i$ "--- значение величины $\alpha_i$, вычисленное алгоритмом $\mathcal{A}$. Если $\beta_i=\Delta$, то алгоритм $\mathcal{A}$ не вычислил значение $\alpha_i$ информационного вектора $\bar\alpha$.
	
	\begin{Def}
		Алгоритм $\mathcal{A}$ называется корректным для задачи $Q$, если выполнено равенство
		\[
		\mathcal{A}(\hat{x},\bar{x})=\bar{\alpha}.
		\]
		Алгоритм $\mathcal{A}$, не являющийся корректным для $Q$, называется некорректным.
	\end{Def}
	
	Далее будем считать, что множество $\{\mathcal{A}\}$ является совокупностью, вообще говоря, некорректных алгоритмов.
	
	\begin{Pred}[аналог теоремы 1 из \cite{Zhuravlev1977}]
		\label{pred:decompositon}
		Каждый алгоритм $\mathcal{A}\in\{\mathcal{A}\}$ представим как последовательность выполнения алгоритмов $R$ и $C$, где $R(\hat{x},\bar{x})=\bar{x}^*$, $\bar{x}^*$ "--- вектор действительных чисел, $C(\bar{x}^*)=\bar{\beta}$, $\beta_i\in\{0,1,\Delta\}$.
	\end{Pred}
	
	\begin{Proof}
		Пусть $D$ "--- алгоритм перехода вектора $\bar{\beta}$ к числовому вектору $\bar{y}$. В качестве $D$ можно рассмотреть, например, $y_i=\beta_i$, если $\beta_i\in\{0,1\}$, и $y_i=1/2$, если $\beta_i=\Delta$. Очевидно, что существует обратный алгоритм $D^{-1}$ перехода от $\bar{y}$ к $\bar{\beta}$. Положим $R=\mathcal{A}{\cdot}D$, $C=D^{-1}$. Тогда очевидно, что $\mathcal{A}=R{\cdot}C=(\mathcal{A}{\cdot}D){\cdot}D^{-1}=\mathcal{A}$.
	\end{Proof}
	
	Из утверждения \ref{pred:decompositon} следует, что множество алгоритмов $\{\mathcal{A}\}$ порождает множества $\{R\}$ и $\{C\}$, которые будем называть \textit{множеством операторов распознавания} и \textit{множеством решающих правил}, соответственно. В качестве операторов из множества $\{R\}$ будем рассматривать операторы $R(\hat{x},\mathcal{Z},\bar{x})$.
	
	\begin{Def}
		Решающее правило $C^*$ называется корректным на множестве входных векторов $X$, если для всякого вектора $\bar{x}$ из $X$ существует хотя бы один числовой вектор $\bar{x}^*$ такой, что $C^*(\bar{x}^*)=\bar{\alpha}$, где $\bar{\alpha}$ "--- информационный вектор входного вектора $\bar{x}$.
	\end{Def}
	
	В множестве операторов $\{R\}$ введём операции умножения на скаляр, сложения и умножения. Пусть $r'$ "--- скаляр, $R',R''\in\{R\}$. Определим операторы $r'{\cdot}R'$, $R'+R''$ и $R{\cdot}R''$ следующим образом:
	
	\begin{equation}
	\label{eq:oper_scalar}
		r'{\cdot}R'=(r'{\cdot}{x_1^*}',\dots,r'{\cdot}{x_l^*}'),
	\end{equation}
	
	\begin{equation}
	\label{eq:oper_sum}
		R'+R''=({x_1^*}'+{x_1^*}'',\dots,{x_1^*}'+{x_l^*}''),
	\end{equation}
	
	\begin{equation}
	\label{eq:oper_mult}
		R'{\cdot}R''=({x_1^*}'{\cdot}{x_1^*}'',\dots,{x_1^*}'{\cdot}{x_l^*}'').
	\end{equation}
	
	\begin{Pred}
		Замыкание $L\{R\}$ множества $\{R\}$ относительно операций \eqref{eq:oper_scalar} и \eqref{eq:oper_sum} является векторным пространством.
	\end{Pred}
	
	\begin{Def}
		Множество $L\{A\}$ алгоритмов $\mathcal{A}=R{\cdot}C^*$ таких, что $R{\in}L\{R\}$, называются линейным замыканием множества $\{\mathcal{A}\}$.
	\end{Def}
	
	Зафиксируем пару $(\hat{x},\bar{x})$ вектора ожидания и входного вектора. Аналогично \cite{Zhuravlev1977} будем рассматривать задачи $Q(\hat{x},\bar{x})$, обладающие следующим свойством относительно множества операторов распознавания $\mathcal{R}$.
	
	\begin{Def}
		Если множество векторов $\{R(\hat{x},\bar{x})\}$, где $R$ пробегает некоторое множество операторов распознавания $\mathcal{R}$, содержит базис в пространстве числовых векторов длины $l$, то задача $Q(\hat{x},\bar{x},\bar{\alpha})$ называется полной относительно $\mathcal{R}$.
	\end{Def}
	
	\begin{Pred}[аналог теоремы 2 из \cite{Zhuravlev1977}]
		\label{pred:correctness}
		Если множество задач $\{Q\}$ состоит лишь из задач, полных относительно $\mathcal R$, то линейное замыкание $L\{R{\cdot}C^*\}$ ($C^*$ "--- произвольное фиксированное корректное решающее правило, $R$ пробегает множество $\mathcal{R}$) является корректным относительно $\{Q\}$.
	\end{Pred}
	
	\begin{Corollary}
		Пусть $\{\mathcal{A}\}$ "--- совокупность некорректных алгоритмов, $\{R\}$ "--- соответствующее множество операторов распознавания, $C^*$ "--- фиксированное корректное решающее правило. Тогда $L\{\mathcal{A}\}=L\{R{\cdot}C^*\}$ является корректным относительно множества задач $\{Q\}$, если $\{Q\}$ состоит из задач, полных относительно $\{R\}$.
	\end{Corollary}
	
	Будем рассматривать только такие задачи $Q(\hat{x},\bar{x},\bar{\alpha})$, для которых удовлетворяется следующее условие: ${\exists}k$ такое, что $x_k$ является $k$-ым элементом вектора $\bar{x}$ и $x_k>1/2$. Такое условие является естественным, иначе вектор $\bar{x}$, в котором отсутствуют веса большие $1/2$, не может рассматриваться как достоверный с точки зрения порогового алгоритма $\mathfrak A_{th}$.
	
	\begin{Theorem}
		\label{th:correctness}
		Линейное замыкание $L\{\mathcal{A}\}$ семейства алгоритмов $\{\mathcal{A}\}=\{R{\cdot}C^*\}$ с произвольным корректным решающим правилом $C^*$ и операторами распознавания $R$, определёнными алгоритмом $\mathfrak A_{th}$, является корректным на множестве задач $\{Q\}$.
	\end{Theorem}

	\begin{Proof}
		В силу утверждения \ref{pred:correctness} достаточно доказать, что произвольная задача $Q\in\{Q\}$ является полной относительно $\{R\}$. Доказательство полноты $Q$ состоит в прямом построении операторов $R_k, k=1,2,\dots,l$ из $L\{R\}$, переводящих пару $(\hat{x},\bar{x}), \hat{x}=(\hat{x}_1,\dots,\hat{x}_l), \bar{x}=(x_1,\dots,x_q)$ в числовой вектор
		\begin{equation} \label{crit:fillness}
			\bar{x}_k^*=(x_{k1}^*,\dots,x_{kl}^*),\ x_{kk}^*=1,\ \forall u\neq k\ x_{ku}^*=0.
		\end{equation} 
		
		Пусть мощность множества $\mathcal Z_k$ признака $f_k$ равна $N$, норма $\|\bar x\|$ равна $M{\leqslant}q$, максимальная компонента вектора $\bar{x}$ равна $x_{max}$. Зафиксируем величину $k$ и коэффициенты $c_1=\min_v\hat x_v, c_2=\frac{M}{1+M}$. Рассмотрим матрицы предсказания из множеств $\mathcal{Z}_1,\dots,\mathcal{Z}_l$ признаков $f_1,\dots,f_l$, удовлетворяющие следующим условиям:
		
		 \begin{enumerate}
		 	\item в каждой матрице предсказаний $Z_r^k\in\mathcal Z_k$ в столбце $\bar{z}_1^r=(z_{11}^r,\dots,z_{1q}^r)$ компонента $z_{1v}^r=1$, если $x_v=x_{max}$, и $z_{1v}^r=0$, если $x_v<x_{max}$; \label{cond:ii}
		 	
		 	\item в каждой матрице предсказаний $Z_r^u\in\mathcal Z_u, u\neq k$ в столбце $\bar{z}_1^r=(z_{11}^r,\dots,z_{1q}^r)$ компонента $z_{1v}^r=0$ при любых $v$. \label{cond:ij}
		 \end{enumerate}
		 
		 Вычислим величину $x_{kk}^*$. Т.~к. $c_1=\min_u\hat x_u$, то условие $\hat x_k\geqslant c_1$ на шаге \ref{alst:select_f} алгоритма $\mathfrak A_{th}$ автоматически выполняется и функция измерения $\hat f_k$ попадает в множество $\hat F^*$. Из условия \ref{cond:ii} следует, что каждая матрица $Z_r^k\in\mathcal Z_k$ попадает в множество $Z^*$ на шаге \ref{alst:select_z} алгоритма $\mathfrak A_{th}$:
		 \[
		 	\frac{\|\bar{z}_1^r-\bar{x}\|}{\|\bar{z}_1^r\|+\|\bar{x}\|}<\frac{\sum_v|z_{1v}^r-x_v|}{1+M}<\frac{M}{1+M}=c_2,
		 \]
		 так как минимум один компонент в $\bar{z}_1^r$ равен $1$ и существует элемент $x_v>1/2$. В этом случае $x_{kk}^*=\gamma{\cdot}N$, где $\gamma$ "--- весовой коэффициент.
		 
		 Вычислим величины $x_{ku}^*$. Т.к. $c_1=\min_v\hat x_v$, то условие $\hat x_u\geqslant c_1$ на шаге \ref{alst:select_f} алгоритма $\mathfrak{A}_{th}$ автоматически выполняется и все функции измерения $\hat f_u$ попадают в множество $\hat F^*$. Из условия \ref{cond:ij} следует, что каждая матрица $Z_r^u\in\mathcal Z_u$ не попадает в множество $Z^*$ на шаге \ref{alst:select_z} алгоритма $\mathfrak A_{th}$:
		 \[
		 \frac{\|\bar{z}_1^r-\bar{x}\|}{\|\bar{z}_1^r\|+\|\bar{x}\|}=\frac{M}{M}=1>\frac{M}{1+M}=c_2.
		 \]
		 В этом случае $x_{ku}^*=0$.
		 
		 Рассмотрим оператор распознавания $\frac{1}{\gamma\cdot N}R_k(\hat x,\mathcal Z^k,\bar x)$, матрицы предсказания которого удовлетворяют условиям \ref{cond:ii}--\ref{cond:ij} и который переводит задачу $Q$ в вектор $\bar x_k^*$, причём $\bar x_{kk}^*=1$, а $\bar x_{ku}^*=0, u\neq k$. Данный оператор удовлетворяет критериям (\ref{crit:fillness}) на вектор $\bar x_k^*$, а значит, необходимый баз в пространстве выходных векторов построен. Полнота задачи $Q$ доказана.
	\end{Proof}
	
	\subsection{Произвольный момент времени}
	Фиксация момента времени не в начале вычислительного цикла, а на~любом другом значении $\tau_s<t<\tau_s+h_i^j$, приводит к~операторам вида $R_i^j(\hat{x}_i^j(t), \mathcal{Z}_i^j, \bar{x}_i^j(t))$, которые кратко будем записывать $R^t$. Для этих операторов постановка задачи распознавания выглядит таким же образом как и для операторов $R$ начального времени: задача $Q^t(\hat{x}_i^j(t), \bar{x}_i^j(t), \bar\alpha)$ состоит в построении алгоритма $\mathcal A^{*t}$, переводящего набор $(\hat{x}_i^j(t), \bar{x}_i^j(t))$ в информационный вектор $\bar\alpha=(\alpha_1,\dots,\alpha_l)$. Определения свойств корректности алгоритма и полноты задачи, а также корректного решающего правила $C^{*t}$, идентичны случаю с начальным моментом времени (Рисунок \ref{fig:rb_correct_statt}). Аналогично, рассматривая только такие задачи $Q^t(\hat{x}_i^j(t), \bar{x}_i^j(t), \bar\alpha)$, в которых имеется как минимум один значимый компонент входного вектора, можно сформулировать следующую теорему (будем далее опускать индексы $i,j$).
	
	\begin{Theorem}
		\label{th:correctness_t}
		Линейное замыкание $L\{\mathcal A^t\}$ семейства алгоритмов $\{\mathcal A^t\}=\{R^t\cdot C^{*t}\}$ с произвольным корректным решающим правилом $C^{*t}$ и операторами распознавания $R^t$, определёнными алгоритмом $\mathfrak A_{th}$, является корректным на множестве задач $\{Q^t\}$.
	\end{Theorem}
	
	\begin{Proof}
		Как и в случае доказательства теоремы \ref{th:correctness} будем строить операторы $R_k^t,\ k=1,2,\dots,l$ из $l\{R^t\}$, переводящие пару $(\hat x(t), \bar x(t))$ в числовой вектор
		\begin{equation}\label{crit:fillness_t}
			\bar x_k^*(t)=(x_{t1,k}^*,\dots,x_{tl,k}^*),\ x_{tk,k}^*=1,\ \forall u\neq k\ x_{tu,k}^*=0.
		\end{equation}
		
		Фиксируя константы $c_1,c_2$ на основании свойств входного вектора и вектора ожиданий и налагая аналогичные условия на матрицы предсказания, но только для $t$-ых столбцов, приходим к построению операторов распознавания $\frac{1}{\gamma}R_k^t(\hat x(t),\mathcal Z^{t,k} \bar x(t))$ ($\gamma$ "--- некоторый весовой коэффициент), выходной вектор которых удовлетворяет критерию (\ref{crit:fillness_t}). Необходимый базис в пространстве выходных векторов построен, полнота задачи $Q^t$ доказана.
	\end{Proof}
	
	\section{Динамические постановки задачи классификации}
	\subsection{Случай одного распознающего блока}
	Теперь рассмотрим динамическую постановку задачи. Будем фиксировать не конкретный момент времени $t$, а промежуток времени ${\Delta}t=[\tau_s,\tau_s+h_i^j)$. В~этом случае распознающий блок $R_i^j$ можно рассматривать как \textit{динамический оператор распознавания} $\hat{R}_i^j(\hat{x}_i^{j+1}(\tau_s), \mathcal{Z}_i^j, \omega_{i\Delta{t}}^j)=\gamma_{i\Delta{t}}^j$, принимающий  функцию входного воздействия $\omega_i^j$, ограниченную на промежутке времени ${\Delta}t$, и выдающий функцию выходной величины $\gamma_i^j$ на том же временном промежутке. Так как мы предполагаем время дискретным, т.~е. множество моментов времени $T_i^j$ является множеством целых чисел, то действие динамического оператора $\hat{R}_i^j$ можно заменить последовательным действием статических операторов $R(\hat{x}_i^{j+1}(\tau_s), \mathcal{Z}_i^j, \bar{x}_i^j(\tau_s)), R^1(\hat{x}_i^j(\tau_s+1), \mathcal{Z}_i^j, \bar{x}_i^j(\tau_s+1)), \dots, R^{h_i^j-1}(\hat{x}_i^j(\tau_s+h_i^j-1), \mathcal{Z}_i^j, \bar{x}_i^j(\tau_s+h_i^j-1))$, в результате выдающих последовательность $\{\bar{x}_i^{*j}(t)\}=\{\bar{x}_i^{*j}(\tau_s), \bar{x}_i^{*j}(\tau_s+1), \dots, \bar{x}_i^{*j}(\tau_s+h_i^j-1)\}$. Так как параметр $h_i^j$ фиксирован, то конечные последовательности векторов  $\omega_{i\Delta{t}}^j$ и $\gamma_{i\Delta{t}}^j$ можно считать матрицами размерности $l_i^j\times{h_i^j}$. Далее будем опускать индексы $i$ и $j$.
	
	Формулировка задачи в динамическом случае будет выглядеть следующим образом: задача $\hat{Q}(\hat{x}, \omega_{{\Delta}t}, \bar{\alpha})$ состоит в построении алгоритма $\hat{\mathcal A}^*$, вычисляющего по поступившему начальному вектору ожиданий $\hat{x}$ матрице входных воздействий $\omega_{{\Delta}t}$  последовательность векторов $\beta_{\Delta{t}}$, монотонно сходящуюся к информационному вектору $\bar{\alpha}$. Т.~е. искомый оператор распознавания $\hat{R}$ должен выдавать матрицу весов присутствия распознаваемых признаков $\gamma_{\Delta{t}}$, столбцы которой должны сходиться (с учётом корректного решающего правила) к информационному вектору: $\lim_{t\to\tau_s+h}\bar{x}^*(t)=\bar{\alpha}$ (Рисунок \ref{fig:rb_correct_dyn}). Введём соответствующие определения.
	
	\begin{figure}[h]
		\centering
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\linewidth,page=3]{rb_correct}
			\caption{для одиночного распознающего блока}
			\label{fig:rb_correct_dyn}
		\end{subfigure}
		\qquad
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\linewidth,page=4]{rb_correct}
			\caption{для случая двухуровневой иерархии}
			\label{fig:rb_correct_hier}
		\end{subfigure}		
		\caption{Динамические схемы корректности.}
	\end{figure}
	
	\begin{Def}
		Алгоритм $\hat{\mathcal{A}}(\hat{x},\bar{x})=\beta_{\Delta{t}}=(\bar{\beta}_1,\dots,\bar{\beta}_h)$ называется корректным для задачи $\hat{Q}$, если выполнено условие
		\[
		\|\bar{\beta}_1-\bar{\alpha}\|\geqslant\|\bar{\beta}_2-\bar{\alpha}\|\geqslant\dots
		\geqslant\|\bar{\beta}_h-\bar{\alpha}\|=0.
		\]
		$\|\bar{\beta}_i-\bar{\alpha}\|=
		\sum_j{(\beta_{ij}-\alpha_j)}$, где $\beta_{ij}-\alpha_j=0$, если $\beta_{ij}=\alpha_j$, $\beta_{ij}-\alpha_j=\frac{1}{2}$, если $\beta_{ij}=\Delta$, и $\beta_{ij}-\alpha_j=1$ иначе. Алгоритм $\hat{\mathcal{A}}$, не являющийся корректным для $\hat{Q}$, называется некорректным.
	\end{Def}
	
	\begin{Pred}
		\label{st:decompositon_dyn}
		Каждый алгоритм $\hat{\mathcal{A}}\in\{\hat{\mathcal{A}}\}$ представим как последовательность выполнения алгоритмов $\hat{R}$ и $\hat{C}$, где $\hat{R}(\hat{x}, \mathcal{Z}, \omega_{\Delta{t}})=\gamma_{\Delta{t}}$, $\gamma_{\Delta{t}}$~---матрица действительных чисел, $\hat{C}(\gamma_{\Delta{t}})=\beta_{\Delta{t}}$, $\beta_{\Delta{t}}$~---матрица значений $\beta_{ij}\in\{0,1,\Delta\}$.
	\end{Pred}
	
	Корректное решающее правило $\hat{C}^*$ для матрицы $\gamma_{\Delta{t}}$ определяется через набор корректных правил для векторов $(
	\hat{C}_1^*, \dots, \hat{C}_h^*)$ таких, что $\|C_1^*(\bar{x}^*(\tau_s))-\bar{\alpha}\|\geqslant\|C_2^*(\bar{x}^*(\tau_s+1))-\bar{\alpha}\|\geqslant\dots\geqslant\|C_h^*(\bar{x}^*(\tau_s+h-1))-\bar{\alpha}\|=0$. В простейшем случае $\forall{i}$ $C_i^*(\bar{x}^*(\tau_s+i))=\bar{\alpha}$ и такое решающее правило будем называть константным. Аналогично статическому случаю вводится определение линейного $L\{\hat{R}\}$ замыкания над множеством $\{\hat{R}\}$. 
	
	\begin{Def}
		Если множество матриц $\{\hat R(\hat x,\omega_{\Delta t})\}$, где $\hat R$ пробегает некоторое множество операторов распознавания $\hat{\mathcal R}$, содержит базис в пространстве числовых матриц размерности $l\times q$, то задача $\hat Q(\hat x,\omega_{\Delta t},\bar{\alpha})$ называется полной относительно $\hat{\mathcal R}$.
	\end{Def}
	
	\begin{Pred}[аналог теоремы 2 из \cite{Zhuravlev1977}]
		\label{pred:correctness_d}
		Если множество задач $\{\hat Q\}$ состоит лишь из задач, полных относительно $\hat{\mathcal R}$, то линейное замыкание $L\{\hat R{\cdot}\hat C^*\}$ ($\hat C^*$ "--- произвольное фиксированное корректное решающее правило, $\hat R$ пробегает множество $\hat{\mathcal R}$) является корректным относительно $\{\hat Q\}$.
	\end{Pred}
		
	Для того, чтобы воспользоваться результатами, полученными при рассмотрении статических операторов $R=R^0$ и $R^t$, необходимо ввести понятие подзадачи.
	
	\begin{Def}
		Если в задаче $Q^t(\hat x(t),\bar x(t),\bar\alpha)$ входной вектор $\bar x(t)$ совпадает с $t$-ым столбцом матрицы $\omega_{\Delta t}$ задачи $\hat Q(\hat x,\omega_{\Delta t},\bar\alpha)$, а вектор $\hat x(t)$ вычисляется на основании алгоритма $\mathfrak A_{th}$ с входным воздействием, равным матрице $\omega_{\Delta t}$, и начальным вектором ожиданий, равным $\hat x$, называется подзадачей задачи $\hat Q$.
	\end{Def}
	
	Зафиксируем начальный вектор ожиданий $\hat{x}$ и последовательность входных векторов $\omega_{\Delta{t}}$. Если, как и в статическом случае, мы будем рассматривать только такие задачи $\hat{Q}(\hat{x},\omega_{\Delta{t}},\bar{\alpha})$, для которых в матрице $\omega_{\Delta{t}}$ в каждом столбце с номером $s$ ${\exists}k$ такое, что $x_{sk}$ является $k$-ым элементом вектора $\bar{x}(\tau_s+s)$ и $x_{sk}>1/2$, то можно сформулировать следующую теорему.
	
	\begin{Theorem}\label{th:correctness_d}
		Линейное замыкание $L\{\hat{\mathcal{A}}\}$ семейства алгоритмов $\{\hat{\mathcal{A}}\}=\{\hat{R}{\cdot}\hat{C}^*\}$ с константным корректным решающим правилом $\hat{C}^*$ и операторами распознавания $\hat{R}$, определёнными алгоритмом $\mathfrak{A}_{th}$, является корректным на~множестве задач $\{\hat{Q}\}$.
	\end{Theorem}
	
	\begin{Proof}
		В силу того, что динамический оператор $\hat{R}$ представим в виде последовательного применения статических операторов $R^t$ к столбцам матрицы $\omega_{\Delta{t}}$, то для доказательства теоремы необходимо подобрать такие операторы $R^t,\ t=0,\dots,h-1$, которые выдают последовательность $\gamma_{\Delta{t}}$, сходящуюся (с учётом применения константного корректного решающего правила $\hat{C}^*=(C_1^*,\dots,C_i^*)$) к информационному вектору $\bar{\alpha}$.
		
		Рассмотрим алгебраическое замыкание $L\{R^t\}$ операторов вида $R^t(\hat{x}(\tau_s+t), \mathcal Z, \omega_{\Delta t}(\tau_s+t))$ с фиксированным вектором $\hat{x}(\tau_s+t)$ и $\omega(\tau_s+t)$. Из задачи $\hat{Q}(\hat{x}, \omega_{{\Delta}t}, \bar{\alpha})$ выделим подзачаду $Q_i(\hat x(\tau_s+t), \omega_{\Delta t}(\tau_s+t),\bar\alpha)$. 

		В силу теорем \ref{th:correctness} и \ref{th:correctness_t} можно построить такой оператор $R^{*t}\in{L}\{R^t\}$, что $C^{*t}\cdot R^{*t}(\omega(\tau_s+i))=\bar{\alpha}$. Формируя таким образом линейные замыкания и выделяя подзадачи для каждого момента времени $t\in[0,h)$, получим необходимую последовательность $\gamma_{\Delta t}=(C^{*1}\cdot R^{*1}(\omega(\tau_s)), \dots, C^{*h}\cdot R^{*h}(\omega(\tau_s+h-1)))=(\bar{\alpha},\dots,\bar{\alpha})$, которая очевидным образом сходится к $\bar{\alpha}$. Корректность, таким образом, доказана.		
	\end{Proof}
	
	\subsection{Случай двухуровневой иерархии распознающих блоков}
	Рассмотрим иерархическую постановку задачи, в которой будет учитываться иерархическая связь между операторами распознавания. Зафиксируем, как и в~динамическом случае, промежуток времени $\Delta t=[\tau_s,\tau_s+h_{i_2}^j)$. Далее, будем рассматривать не единичный распознающий блок, а двухуровневую иерархию $E_j^2$, на каждом уровне которой будет по одному распознающему блоку $R_{i_1}^{j+1}$ и $R_{i_2}^j$. Данную иерархию можно рассматривать как \textit{иерархический оператор распознавания} $\hat R_{e,j}^2(\hat x_{i_1}^{j+1}(\tau_s),\mathcal Z_{i_1}^{j+1},\mathcal Z_{i_2}^j,\omega_{i_2\Delta t}^j)=\bar x_{i_1}^{*j+1}$, принимающий функцию входного воздействия $\omega_{i_2\Delta t}^j$ нижнего уровня, ограниченную на промежутке времени $\Delta t$, и выдающий весовой вектор присутствия распознаваемых признаков $\bar x_{i_1}^{*j+1}$. Т.~к. в иерархии $E_j^2$ вектор состояния блока $R_{i_1}^{j+1}$ является одновременно и вектором ожидания для блока $R_{i_2}^j$, а конечный выходной вектор $\bar x_{i_2}^{*j}$ "--- входным вектором $\bar x_{i_1}^{j+1}$, то действие иерархического оператора $\hat R_{e,j}^2$ можно заменить последовательным действием динамического оператора $\hat R_{i_2}^j(\hat x _{i_2}^{j+1},\mathcal Z_{i_2}^j,\omega_{i_2\Delta t}^j)$ нижнего уровня и статического оператора $R_{i_1}^{j+1,t}(\hat x _{i_1}^{j+1},\mathcal Z_{i_1}^{j+1},\bar x_{i_1}^{j+1}(\tau_s))$ верхнего уровня, где $t$ является моментом времени текущего вычислительного цикла распознающего блока $R_{i_1}^{j+1}$, соответствующему моменту времени $\tau_s$ для распознающего блока $R_{i_2}^j$.
	
	Формулировка задачи в~иерархическом случае будет выглядеть следующим образом: задача $\hat Q_{e,j}^2(\hat x_{i_1}^{j+2},\omega_{i_2\Delta t}^j,\bar\alpha_{i_1}^{j+1})$ состоит в построении алгоритма $\hat{\mathcal A_e}$, вычисляющего по поступившему начальному вектору ожиданий $\hat x_{i_1}^{j+2}$ и матрице входных воздействий $\omega_{i_2\Delta t}^j$ значения информационного вектора $\bar\alpha_{i_1}^{j+1}$ (Рисунок \ref{fig:rb_correct_hier}). Определения свойств корректности алгоритма и полноты задачи, а также корректного решающего правила, в данном случае в~точности совпадают с аналогичными определениями для статического случая.
	
	Зафиксируем начальный вектор ожиданий $\hat x_{i_1}^{j+2}$ и последовательность входных векторов $\omega_{i_2\Delta{t}}^j$. Если мы будем рассматривать только такие задачи $\hat Q_{e,j}^2(\hat x_{i_1}^{j+2},\omega_{i_2\Delta{t}}^j,\bar\alpha_{i_1}^{j+1})$, для которых в матрице $\omega_{i_2\Delta{t}}^j$ в каждом столбце с номером $s$ ${\exists}k$ такое, что $x_{sk}$ является $k$-ым элементом вектора $\bar x_{i_2}^j(\tau_s+s)$ и $x_{sk}>1/2$, то можно сформулировать следующую теорему.
		
	\begin{Theorem}
		Линейное замыкание $L\{\hat{\mathcal A_e}\}$ семейства алгоритмов $\{\hat{\mathcal A}_e\}=\{\hat R_{e,j}^2\cdot\hat C_e^*\}$ с произвольным корректным решающим правилом $\hat C_e^*$ и операторами распознавания $\hat R_{e,j}^2$, определёнными алгоритмом $\mathfrak A_{th}$, является корректным на~множестве задач $\{\hat Q_{e,j}^2\}$.
	\end{Theorem}
	
	\begin{Proof}
		Доказательство корректности в данном случае сводится к формулировке подзадачи нижнего уровня $\hat Q_2(\hat x_{i_2}^{j+1},\omega_{i_2\Delta t}^j,\bar\alpha_{i_2}^j)$. Т.~е. необходимо сформировать по~задаче $\hat Q_{e,j}^2$ информационный вектор $\bar\alpha_{i_2}^j$ и вектор ожидания $\hat x_{i_2}^{j+1}$.
		
		Следую определению вычислительного цикла в алгоритме $\mathfrak A_{th}$, будем считать, что $\hat x_{i_2}^{j+1}$ равен тому состоянию распознающего блока $R_{i_1}^{j+1}$,~которое было вычислено к моменту времени $\tau_s$, т.~е. вектору $\hat x_{i_1}^{j+1}$. Каждый компонент $\alpha_{i_2u}^j$ информационного вектора $\bar\alpha_{i_2}^j$ будем вычислять по следующему правилу:
		\[
			\alpha_{i_2u}^j=\begin{cases}
				1, & \text{если $\sum\limits_{v=1}^{l_{i_1}^{j+1}}\frac{\alpha_{i_1v}^{j+1}}{|\mathcal{Z}_v|}\sum\limits_{w=1}^{|\mathcal{Z}_v|}z_{1v}^w>0$,}\\
				
				0, & \text{иначе.}
			\end{cases}
		\]
		Т.к. входной вектор распознающего блока $R_{i_1}^j$ равен вектору $\bar\alpha_{i_2}^j$, то такие значения компонентов информационного вектора позволяют удовлетворить ограничениям теоремы \ref{th:correctness_t} (существование такого компонента входного вектора, который бы имел значение большее $1/2$). С другой стороны, формулируя задачу $\hat Q_2(\hat x_{i_2}^{j+1},\omega_{i_2\Delta t}^j,\bar\alpha_{i_2}^j)$ мы попадаем в условия теоремы \ref{th:correctness_d}. Пользуясь результатами этих теорем, мы приходим к выводу, что среди алгоритмов линейного замыкания $L\{\hat{\mathcal A_e}\}$ имеется оператор, переводящий пару $(\hat x_{i_1}^{j+1},\omega_{i_2\Delta t}^j)$ в информационный вектор $\bar\alpha_{i_1}^{j+1}$.
	\end{Proof}
	
	\section*{Заключение}
	Исследованы алгебраические свойства множества распознающих блоков. Показано, что динамические характеристики образной компоненты описываются в терминах классической теории управления. Построены операторы распознавания,~позволяющие выполнить постановки задач классификации в терминах алгебраической теорий распознавания. Установлено, что линейные замыкания операторов распознавания, которые строятся в статических и динамических случаях, обладают свойством корректности относительно входных данных и требуемых результатов классификации. Это означает существования такого процесса обучения, в рамках которого будет сформирована иерархия образных компонент, корректно распознающая (классифицирующая) поступающие сигналы.
	
	\section*{Благодарности}
	Исследование выполнено при финансовой поддержке РФФИ в рамках инициативного проекта \No 14-07-00611 а.
	
%	\nocite{*}
	\inputencoding{cp1251}
	\bibliography{../biblio/main}
	\inputencoding{utf8}
\end{document}