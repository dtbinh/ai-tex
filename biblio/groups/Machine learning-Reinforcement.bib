Automatically generated by Mendeley Desktop 1.17.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Zhang2017,
author = {Zhang, Fengyun and Duan, Shukai and Wang, Lidan},
doi = {10.1007/s11571-017-9423-7},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Neurodynamics/2017/Zhang, Duan, Wang - 2017.pdf:pdf},
issn = {1871-4080},
journal = {Cognitive Neurodynamics},
keywords = {Route searching,Neural network,Heuristic reinforce,exploitation,network {\'{a}} heuristic,reinforcement learning {\'{a}} greedy,route searching {\'{a}} neural},
publisher = {Springer Netherlands},
title = {{Route searching based on neural networks and heuristic reinforcement learning}},
url = {http://link.springer.com/10.1007/s11571-017-9423-7},
year = {2017}
}
@article{Sun2005,
abstract = {This article explicates the interaction between implicit and explicit processes in skill learning, in contrast to the tendency of researchers to study each type in isolation. It highlights various effects of the interaction on learning (including synergy effects). The authors argue for an integrated model of skill learning that takes into account both implicit and explicit processes. Moreover, they argue for a bottom-up approach (first learning implicit knowledge and then explicit knowledge) in the integrated model. A variety of qualitative data can be accounted for by the approach. A computational model, CLARION, is then used to simulate a range of quantitative data. The results demonstrate the plausibility of the model, which provides a new perspective on skill learning.},
author = {Sun, Ron and Slusarz, Paul and Terry, Chris},
doi = {10.1037/0033-295X.112.1.159},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Psychological Review/2005/Sun, Slusarz, Terry - 2005.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471},
journal = {Psychological Review},
keywords = {artificial grammars,connectionist,decision-making,information,memory,model,nonconscious acquisition,procedural knowledge,tacit knowledge,task-performance},
number = {1},
pages = {159--192},
pmid = {15631592},
title = {{The Interaction of the Explicit and the Implicit in Skill Learning: A Dual-Process Approach.}},
volume = {112},
year = {2005}
}
@incollection{Ponsen2010,
abstract = {In this paper we survey the basics of reinforcement learning, gener- alization and abstraction. We start with an introduction to the fundamentals of reinforcement learning and motivate the necessity for generalization and abstrac- tion. Next we summarize themost important techniques available to achieve both generalization and abstraction in reinforcement learning. We discuss basic func- tion approximation techniques and delve into hierarchical, relational and transfer learning. All concepts and techniques are illustrated with examples.},
author = {Ponsen, Marc and Taylor, Matthew E. and Tuyls, Karl},
booktitle = {Adaptive and Learning Agents},
doi = {10.1007/978-3-642-11814-2_1},
editor = {Taylor, Matthew E. and Tuyls, Karl},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Adaptive and Learning Agents/2010/Ponsen, Taylor, Tuyls - 2010.pdf:pdf},
isbn = {3642118135},
issn = {03029743},
pages = {1--32},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Abstraction and generalization in reinforcement learning: A summary and framework}},
year = {2010}
}
@article{Duan2016,
abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophis-ticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, bene-fiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a " fast " reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL 2 , the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (" slow ") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including ob-servations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the " fast " RL algorithm on the current (previously unseen) MDP. We evaluate RL 2 experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL 2 is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL 2 on a vision-based navigation task and show that it scales up to high-dimensional problems.},
archivePrefix = {arXiv},
arxivId = {1611.02779},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter and Sutskever, Ilya and Abbeel, Pieter},
doi = {10.1051/0004-6361/201527329},
eprint = {1611.02779},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/arXiv/2016/Duan et al. - 2016.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv},
pages = {1--14},
title = {{RL{\^{}}2: Fast Reinforcement Learning Via Slow Reinforcement Learning}},
year = {2016}
}
@inproceedings{Lange2010,
author = {Lange, Sascha and Riedmiller, Martin},
booktitle = {The 2010 International Joint Conference on Neural Networks (IJCNN)},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/The 2010 International Joint Conference on Neural Networks (IJCNN)/2010/Lange, Riedmiller - 2010.pdf:pdf},
pages = {1--8},
publisher = {IEEE},
title = {{Deep Auto-Encoder Neural Networks in Reinforcement Learning}},
year = {2010}
}
@article{Fu2006,
abstract = {The authors propose a reinforcement-learning mechanism as a model for recurrent choice and extend it to account for skill learning. The model was inspired by recent research in neurophysiological studies of the basal ganglia and provides an integrated explanation of recurrent choice behavior and skill learning. The behavior includes effects of differential probabilities, magnitudes, variabilities, and delay of reinforcement. The model can also produce the violation of independence, preference reversals, and the goal gradient of reinforcement in maze learning. An experiment was conducted to study learning of action sequences in a multistep task. The fit of the model to the data demonstrated its ability to account for complex skill learning. The advantages of incorporating the mechanism into a larger cognitive architecture are discussed.},
author = {Fu, Wai-Tat and Anderson, John R},
doi = {10.1037/0096-3445.135.2.184},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Journal of experimental psychology. General/2006/Fu, Anderson - 2006.pdf:pdf},
isbn = {0096-3445$\backslash$n1939-2222},
issn = {0096-3445},
journal = {Journal of experimental psychology. General},
keywords = {Animals,Basal Ganglia,Basal Ganglia: physiology,Choice Behavior,Cognition,Cognition: physiology,Humans,Maze Learning,Models, Psychological,Probability,Rats,Reinforcement (Psychology),Reinforcement Schedule,Serial Learning},
number = {2},
pages = {184--206},
pmid = {16719650},
title = {{From recurrent choice to skill learning: a reinforcement-learning model.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16719650},
volume = {135},
year = {2006}
}
@misc{Brockam2016,
abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
booktitle = {ArXiv: 1606.01540},
eprint = {1606.01540},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/ArXiv 1606.01540/2016/Brockman et al. - 2016.pdf:pdf},
pages = {1--4},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
archivePrefix = {arXiv},
arxivId = {1412.3409},
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
doi = {10.1023/A:1022676722315},
eprint = {1412.3409},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Machine Learning/1992/Watkins, Dayan - 1992.pdf:pdf},
isbn = {978-1-4613-6608-9},
issn = {15730565},
journal = {Machine Learning},
keywords = {(Formula presented.)-learning,asynchronous dynamic programming,reinforcement learning,temporal differences},
number = {3},
pages = {279--292},
pmid = {7761831},
title = {{Q-Learning}},
volume = {8},
year = {1992}
}
@article{Li2017,
abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We start with background of deep learning and reinforcement learning, as well as introduction of testbeds. Next we discuss Deep Q-Network (DQN) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised learning, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, spoken dialogue systems (a.k.a. chatbot), machine translation, text sequence prediction, neural architecture design, personalized web services, healthcare, finance, and music generation. We mention topics/papers not reviewed yet. After listing a collection of RL resources, we close with discussions.},
archivePrefix = {arXiv},
arxivId = {1701.07274},
author = {Li, Yuxi},
eprint = {1701.07274},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2017/Li - 2017.pdf:pdf},
pages = {1--30},
title = {{Deep Reinforcement Learning: An Overview}},
url = {http://arxiv.org/abs/1701.07274},
year = {2017}
}
@article{Levine2016,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1504.00702},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Journal of Machine Learning Research/2016/Levine et al. - 2016.pdf:pdf},
isbn = {9781479969227},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Neural Networks,Optimal Control,Reinforcement Learning,Vision},
pages = {1--40},
pmid = {15003161},
title = {{End-to-End Training of Deep Visuomotor Policies}},
volume = {17},
year = {2016}
}
@incollection{Riedmiller2005,
abstract = {This paper introduces NFQ, an algorithm for efficient and ef- fective training of a Q-value function represented by amulti-layer percep- tron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
author = {Riedmiller, Martin},
booktitle = {Machine Learning: ECML 2005},
doi = {10.1007/11564096_32},
editor = {Gama, Jo{\~{a}}o and Camacho, Rui and Brazdil, Pavel B. and Jorge, Al{\'{i}}pio M{\'{a}}rio and Torgo, Lu{\'{i}}s},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Machine Learning ECML 2005/2005/Riedmiller - 2005.pdf:pdf},
isbn = {3540292438},
issn = {03029743},
pages = {317--328},
publisher = {Springer Berlin Heidelberg},
title = {{Neural Fitted Q Iteration – First Experiences with a Data Efficient Neural Reinforcement Learning Method}},
url = {http://link.springer.com/10.1007/11564096{\_}32},
year = {2005}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task involving finding rewards in random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/arXiv/2016/Mnih et al. - 2016.pdf:pdf},
isbn = {9781510829008},
journal = {arXiv},
pages = {1--28},
pmid = {1000272564},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@article{Kaelbling1996,
abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
doi = {10.1613/jair.301},
eprint = {9605103},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Journal of Artificial Intelligence Research/1996/Kaelbling, Littman, Moore - 1996.pdf:pdf},
isbn = {0-7803-3213-X},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {237--285},
pmid = {17255001},
primaryClass = {cs},
title = {{Reinforcement learning: A survey}},
volume = {4},
year = {1996}
}
@article{Wang2016,
abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
archivePrefix = {arXiv},
arxivId = {1611.05763},
author = {Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
eprint = {1611.05763},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2016/Wang et al. - 2016.pdf:pdf},
month = {nov},
pages = {1--17},
title = {{Learning to reinforcement learn}},
url = {http://arxiv.org/abs/1611.05763},
year = {2016}
}
@inproceedings{Dickens2010,
author = {Dickens, Luke and Broda, Krysia and Russo, Alessandra},
booktitle = {ECAI 2010: 19th European Conference on Artificial Intelligence, 16-20 August 2010, Lisbon, Portugal : Including Prestigious Applications of Artificial Intelligence (PAIS-2010) : Proceedings},
doi = {10.3233/978-1-60750-606-5-367},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/ECAI 2010 19th European Conference on Artificial Intelligence, 16-20 August 2010, Lisbon, Portugal Including Prestigious Applications of Ar./2010/Dickens, Broda, Russo - 2010.pdf:pdf},
isbn = {9781607506065},
pages = {367--372},
title = {{The Dynamics of Multi-Agent Reinforcement Learning}},
year = {2010}
}
@book{Sutton2011,
address = {М.},
author = {Саттон, Р.С. and Барто, Э. Г.},
edition = {2-е},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2011/Саттон, Барто - 2011.pdf:pdf},
language = {russian},
pages = {399},
publisher = {БИНОМ. Лаборатория знаний},
title = {{Обучение с подкреплением}},
year = {2011}
}
@misc{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
booktitle = {arXiv: 1312.5602},
eprint = {1312.5602},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/arXiv 1312.5602/2013/Mnih et al. - 2013.pdf:pdf},
pages = {1--9},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@book{Sun2001,
abstract = {This paper presents a skill learning model Clarion. Different from existing models of mostly high-level skill learning that use a top-down approach (that is, turning declarative knowledge into procedural knowledge through practice), we adopt a bottom-up approach toward low-level skill learning, where procedural knowledge develops first and declarative knowledge develops later. Our model is formed by integrating connectionist, reinforcement, and symbolic learning methods to perform on-line reactive learning. It adopts a two-level dual-representation framework (Sun, 1995), with a combination of localist and distributed representation. We compare the model with human data in a minefield navigation task, demonstrating some match between the model and human data in several respects. ?? 2001 Cognitive Science Society, Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sun, Ron and Merrill, Edward and Peterson, Todd},
booktitle = {Cognitive Science},
doi = {10.1016/S0364-0213(01)00035-0},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Science/2001/Sun, Merrill, Peterson - 2001.pdf:pdf},
isbn = {1573884766},
issn = {03640213},
number = {2},
pages = {203--244},
pmid = {15003161},
title = {{From implicit skills to explicit knowledge: A bottom-up model of skill learning}},
volume = {25},
year = {2001}
}
