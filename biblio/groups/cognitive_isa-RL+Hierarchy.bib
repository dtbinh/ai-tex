Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Botvinick2009a,
abstract = {Research on human and animal behavior has long emphasized its hierarchical structure-the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andrew C.},
doi = {10.1016/j.cognition.2008.08.011},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognition/2009/Botvinick, Niv, Barto - 2009.pdf:pdf},
issn = {1873-7838},
journal = {Cognition},
keywords = {Animals,Humans,Models,Nerve Net,Nerve Net: physiology,Prefrontal Cortex,Prefrontal Cortex: physiology,Problem Solving,Problem Solving: physiology,Psychological,Reinforcement (Psychology)},
number = {3},
pages = {262--80},
pmid = {18926527},
title = {{Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2783353{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {113},
year = {2009}
}
@article{Frank2012,
abstract = {Growing evidence suggests that the prefrontal cortex (PFC) is organized hierarchically, with more anterior regions having increasingly abstract representations. How does this organization support hierarchical cognitive control and the rapid discovery of abstract action rules? We present computational models at different levels of description. A neural circuit model simulates interacting corticostriatal circuits organized hierarchically. In each circuit, the basal ganglia gate frontal actions, with some striatal units gating the inputs to PFC and others gating the outputs to influence response selection. Learning at all of these levels is accomplished via dopaminergic reward prediction error signals in each corticostriatal circuit. This functionality allows the system to exhibit conditional if-then hypothesis testing and to learn rapidly in environments with hierarchical structure. We also develop a hybrid Bayesian-reinforcement learning mixture of experts (MoE) model, which can estimate the most likely hypothesis state of individual participants based on their observed sequence of choices and rewards. This model yields accurate probabilistic estimates about which hypotheses are attended by manipulating attentional states in the generative neural model and recovering them with the MoE model. This 2-pronged modeling approach leads to multiple quantitative predictions that are tested with functional magnetic resonance imaging in the companion paper.},
author = {Frank, Michael J. and Badre, David},
doi = {10.1093/cercor/bhr114},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cerebral cortex (New York, N.Y. 1991)/2012/Frank, Badre - 2012.pdf:pdf},
issn = {1460-2199},
journal = {Cerebral cortex (New York, N.Y. : 1991)},
keywords = {Computer Simulation,Corpus Striatum,Corpus Striatum: cytology,Corpus Striatum: physiology,Humans,Learning,Learning: physiology,Models,Neural Pathways,Neural Pathways: cytology,Neural Pathways: physiology,Neurological,Neurons,Neurons: physiology,Prefrontal Cortex,Prefrontal Cortex: cytology,Prefrontal Cortex: physiology,Reinforcement (Psychology)},
number = {3},
pages = {509--26},
pmid = {21693490},
title = {{Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: Computational analysis}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3278315{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {22},
year = {2012}
}
@article{Botvinick2012,
abstract = {The hierarchical structure of human and animal behavior has been of critical interest in neuroscience for many years. Yet understanding the neural processes that give rise to such structure remains an open challenge. In recent research, a new perspective on hierarchical behavior has begun to take shape, inspired by ideas from machine learning, and in particular the framework of hierarchical reinforcement learning. Hierarchical reinforcement learning builds on traditional reinforcement learning mechanisms, extending them to accommodate temporally extended behaviors or subroutines. The resulting computational paradigm has begun to influence both theoretical and empirical work in neuroscience, conceptually aligning the study of hierarchical behavior with research on other aspects of learning and decision making, and giving rise to some thought-provoking new findings. ?? 2012.},
author = {Botvinick, Matthew Michael},
doi = {10.1016/j.conb.2012.05.008},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Current Opinion in Neurobiology/2012/Botvinick - 2012.pdf:pdf},
isbn = {0818653302},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
number = {6},
pages = {956--962},
pmid = {22695048},
publisher = {Elsevier Ltd},
title = {{Hierarchical reinforcement learning and decision making}},
url = {http://dx.doi.org/10.1016/j.conb.2012.05.008},
volume = {22},
year = {2012}
}
@inproceedings{Rasmussen1998,
author = {Rasmussen, Daniel and Eliasmith, Chris},
booktitle = {Proceedings of the 36th Annual Conference of the Cognitive Science Society},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the 36th Annual Conference of the Cognitive Science Society/2014/Rasmussen, Eliasmith - 2014.pdf:pdf},
keywords = {brain could achieve its,hierarchical,hrl into a theory,neural engineering framework,neural model,of neural function,providing a hypothe-,reinforcement learning,sis for how the,strengths in scaling},
number = {1},
pages = {1252--1257},
title = {{A neural model of hierarchical reinforcement learning}},
year = {2014}
}
@phdthesis{Rasmussen2014,
author = {Rasmussen, Daniel},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2014/Rasmussen - 2014.pdf:pdf},
pages = {175},
school = {Unversetu of Waterloo},
title = {{Hierarchical reinforcement learning in a biologically plausible neural architecture}},
year = {2014}
}
