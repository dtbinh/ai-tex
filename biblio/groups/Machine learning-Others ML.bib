Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Schein2015,
abstract = {We present a Bayesian tensor factorization model for inferring latent group structures from dynamic pairwise interaction patterns. For decades, political scientists have collected and analyzed records of the form “country i took action a toward country j at time t”—known as dyadic events—in order to form and test theories of international relations. We represent these event data as a tensor of counts and develop Bayesian Poisson tensor factorization to infer a lowdimensional, interpretable representation of their salient patterns. We demonstrate that our model's predictive performance is better than that of standard non-negative tensor factorization methods. We also provide a comparison of our variational updates to their maximum likelihood counterparts. In doing so, we identify a better way to form point estimates of the latent factors than that typically used in Bayesian Poisson matrix factorization. Finally, we showcase our model as an exploratory analysis tool for political scientists. We show that the inferred latent factor matrices capture interpretable multilateral relations that both conform to and inform our knowledge of international affairs.},
archivePrefix = {arXiv},
arxivId = {1506.03493},
author = {Schein, Aaron and Paisley, John and Blei, David M and Wallach, Hanna},
doi = {10.1145/2783258.2783414},
eprint = {1506.03493},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining/2015/Schein et al. - 2015.pdf:pdf},
isbn = {9781450336642},
issn = {9781450336642},
journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {all or part of,bayesian inference,dyadic data,international relations,or hard copies of,permission to make digital,poisson tensor factorization,this work for per-},
pages = {1045--1054},
title = {{Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts}},
url = {http://www.cs.columbia.edu/{~}blei/papers/ScheinPaisleyBleiWallach2015.pdf},
year = {2015}
}
@article{Wiering1997,
author = {Wiering, M. and Schmidhuber, J.},
doi = {10.1177/105971239700600202},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Adaptive Behavior/1997/Wiering, Schmidhuber - 1997.pdf:pdf},
isbn = {1059712397006},
issn = {1059-7123},
journal = {Adaptive Behavior},
keywords = {hierarchical q-learning,non-markov,pomdps,reinforcement learning,subgoal learning},
number = {2},
pages = {219--246},
title = {{HQ-Learning}},
url = {http://adb.sagepub.com/cgi/doi/10.1177/105971239700600202},
volume = {6},
year = {1997}
}
@article{Tamar2016,
abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
archivePrefix = {arXiv},
arxivId = {1602.02867},
author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
eprint = {1602.02867},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/arXiv/2016/Tamar, Levine, Abbeel - 2016.pdf:pdf},
journal = {arXiv},
month = {feb},
pages = {1--14},
title = {{Value Iteration Networks}},
url = {http://arxiv.org/abs/1602.02867},
year = {2016}
}
@article{Andrychowicz2016a,
abstract = {In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory. We show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples. In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training. We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.},
archivePrefix = {arXiv},
arxivId = {1602.03218},
author = {Andrychowicz, Marcin and Kurach, Karol},
eprint = {1602.03218},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/International Conference on Machine Learning (ICML)/2016/Andrychowicz, Kurach - 2016.pdf:pdf},
journal = {International Conference on Machine Learning (ICML)},
keywords = {ICML,attention,machine learning},
title = {{Learning Efficient Algorithms with Hierarchical Attentive Memory}},
url = {http://arxiv.org/abs/1602.03218},
year = {2016}
}
@article{Hochreiter2001,
author = {Hochreiter, Sepp and Younger, A Steven and Conwell, Peter R},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2001/Hochreiter, Younger, Conwell - 2001.pdf:pdf},
pages = {87--94},
title = {{Learning to Learn Using Gradient Descent}},
year = {2001}
}
