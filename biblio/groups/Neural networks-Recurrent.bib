Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{MacNeil2011,
abstract = {A central criticism of standard theoretical approaches to constructing stable, recurrent model networks is that the synaptic connection weights need to be finely-tuned. This criticism is severe because proposed rules for learning these weights have been shown to have various limitations to their biological plausibility. Hence it is unlikely that such rules are used to continuously fine-tune the network in vivo. We describe a learning rule that is able to tune synaptic weights in a biologically plausible manner. We demonstrate and test this rule in the context of the oculomotor integrator, showing that only known neural signals are needed to tune the weights. We demonstrate that the rule appropriately accounts for a wide variety of experimental results, and is robust under several kinds of perturbation. Furthermore, we show that the rule is able to achieve stability as good as or better than that provided by the linearly optimal weights often used in recurrent models of the integrator. Finally, we discuss how this rule can be generalized to tune a wide variety of recurrent attractor networks, such as those found in head direction and path integration systems, suggesting that it may be used to tune a wide variety of stable neural systems.},
author = {MacNeil, David and Eliasmith, Chris},
doi = {10.1371/journal.pone.0022885},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/PLoS ONE/2011/MacNeil, Eliasmith - 2011.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pmid = {21980334},
title = {{Fine-tuning and the stability of recurrent neural networks}},
volume = {6},
year = {2011}
}
@techreport{Jaeger2014a,
archivePrefix = {arXiv},
arxivId = {arXiv:1403.3369v1},
author = {Jaeger, Herbert},
eprint = {arXiv:1403.3369v1},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2014/Jaeger - 2014(2).pdf:pdf},
institution = {Jacobs University Bremen},
title = {{Controlling Recurrent Neural Networks by Conceptors}},
year = {2014}
}
@article{Graves2006,
abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
doi = {10.1145/1143844.1143891},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the 23rd international conference on Machine Learning/2006/Graves et al. - 2006.pdf:pdf},
isbn = {1595933832},
issn = {10987576},
journal = {Proceedings of the 23rd international conference on Machine Learning},
pages = {369--376},
title = {{Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks}},
year = {2006}
}
@article{Young2014,
abstract = {Deep machine learning offers a comprehensive framework for extracting meaningful features from complex observations in an unsupervised manner. The majority of deep learning architectures described in the literature primarily focus on extracting spatial features. However, in real-world settings, capturing temporal dependencies in observations is critical for accurate inference. This paper introduces an enhancement to DeSTIN - a compositional deep learning architecture in which each layer consists of multiple instantiations of a common node - that learns to represent spatiotemporal patterns in data based on a novel recurrent clustering algorithm. Contrary to mainstream deep architectures, such as deep belief networks where layer-by-layer training is assumed, each of the nodes in the proposed architecture is trained independently and in parallel. Moreover, top-down and bottom-up information flows facilitate rich feature formation. A semi-supervised setting is demonstrated achieving state-of-the-art results on the MNIST classification benchmarks. A GPU implementation is discussed further accentuating the scalability properties of the proposed framework. ?? 2013 Elsevier Ltd. All rights reserved.},
author = {Young, S. R. and Davis, A. and Mishtal, A. and Arel, I.},
doi = {10.1016/j.patrec.2013.07.013},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Pattern Recognition Letters/2014/Young et al. - 2014.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Deep machine learning,Online clustering,Pattern recognition,Recurrent clustering,Spatiotemporal signals,Unsupervised feature extraction},
number = {1},
pages = {115--123},
publisher = {Elsevier B.V.},
title = {{Hierarchical spatiotemporal feature extraction using recurrent online clustering}},
url = {http://dx.doi.org/10.1016/j.patrec.2013.07.013},
volume = {37},
year = {2014}
}
