Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@incollection{Marek2008,
address = {Berlin},
author = {Marek, Rudolf and Skrbek, Miroslav},
booktitle = {Artificial Neural Networks - ICANN 2008},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Artificial Neural Networks - ICANN 2008/2008/Marek, Skrbek - 2008.pdf:pdf},
pages = {159--168},
publisher = {Springer},
title = {{Efficient Implementation of the THSOM Neural Network}},
year = {2008}
}
@inproceedings{Koutnik,
abstract = {In this paper we present a new self-organizing neural network, which builds a spatiotemporal model of an input temporal sequence inductively. The network is an extension of Kohonen's Self-organizing Map with a modified Hebb's rule for update of temporal synapses. The model building behavior is shown on inductive learning of a transition matrix from a data generated by a Markov Chain.},
author = {Koutnik, Jan},
booktitle = {Proceeding of International Workshop on Inductive Modelling (IWIM 2007)},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceeding of International Workshop on Inductive Modelling (IWIM 2007)/2007/Koutnik - 2007.pdf:pdf},
keywords = {inductive modelling,self-organization,temporal sequences},
pages = {269--277},
title = {{Inductive Modelling of Temporal Sequences by Means of Self-organization}},
year = {2007}
}
@article{Graves2006,
abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
doi = {10.1145/1143844.1143891},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the 23rd international conference on Machine Learning/2006/Graves et al. - 2006.pdf:pdf},
isbn = {1595933832},
issn = {10987576},
journal = {Proceedings of the 23rd international conference on Machine Learning},
pages = {369--376},
title = {{Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks}},
year = {2006}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.7828v1},
author = {Schmidhuber, J},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {arXiv:1404.7828v1},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neural Networks/2015/Schmidhuber - 2015.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
pages = {85--117},
publisher = {Elsevier Ltd},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://arxiv.org/abs/1404.7828},
volume = {61},
year = {2015}
}
@article{Beck2007,
abstract = {From first principles, we derive a quadratic nonlinear, first-order dynamical system capable of performing exact Bayes-Markov inferences for a wide class of biologically plausible stimulus-dependent patterns of activity while simultaneously providing an online estimate of model performance. This is accomplished by constructing a dynamical system that has solutions proportional to the probability distribution over the stimulus space, but with a constant of proportionality adjusted to provide a local estimate of the probability of the recent observations of stimulus-dependent activity-given model parameters. Next, we transform this exact equation to generate nonlinear equations for the exact evolution of log likelihood and log-likelihood ratios and show that when the input has low amplitude, linear rate models for both the likelihood and the log-likelihood functions follow naturally from these equations. We use these four explicit representations of the probability distribution to argue that, in contrast to the arguments of previous work, the dynamical system for the exact evolution of the likelihood (as opposed to the log likelihood or log-likelihood ratios) not only can be mapped onto a biologically plausible network but is also more consistent with physiological observations.},
author = {Beck, Jeffrey M and Pouget, Alexandre},
doi = {10.1162/neco.2007.19.5.1344},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neural computation/2007/Beck, Pouget - 2007.pdf:pdf},
isbn = {0899-7667 (Print)},
issn = {0899-7667},
journal = {Neural computation},
number = {5},
pages = {1344--1361},
pmid = {17381269},
title = {{Exact inferences in a neural implementation of a hidden Markov model}},
volume = {19},
year = {2007}
}
@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00344251},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Biological Cybernetics/1980/Fukushima - 1980.pdf:pdf},
isbn = {0340-1200},
issn = {03401200},
journal = {Biological Cybernetics},
number = {4},
pages = {193--202},
pmid = {7370364},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
volume = {36},
year = {1980}
}
@article{MacNeil2011,
abstract = {A central criticism of standard theoretical approaches to constructing stable, recurrent model networks is that the synaptic connection weights need to be finely-tuned. This criticism is severe because proposed rules for learning these weights have been shown to have various limitations to their biological plausibility. Hence it is unlikely that such rules are used to continuously fine-tune the network in vivo. We describe a learning rule that is able to tune synaptic weights in a biologically plausible manner. We demonstrate and test this rule in the context of the oculomotor integrator, showing that only known neural signals are needed to tune the weights. We demonstrate that the rule appropriately accounts for a wide variety of experimental results, and is robust under several kinds of perturbation. Furthermore, we show that the rule is able to achieve stability as good as or better than that provided by the linearly optimal weights often used in recurrent models of the integrator. Finally, we discuss how this rule can be generalized to tune a wide variety of recurrent attractor networks, such as those found in head direction and path integration systems, suggesting that it may be used to tune a wide variety of stable neural systems.},
author = {MacNeil, David and Eliasmith, Chris},
doi = {10.1371/journal.pone.0022885},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/PLoS ONE/2011/MacNeil, Eliasmith - 2011.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pmid = {21980334},
title = {{Fine-tuning and the stability of recurrent neural networks}},
volume = {6},
year = {2011}
}
@article{Hinton2006,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neural Computation/2006/Hinton, Osindero, Teh - 2006.pdf:pdf},
journal = {Neural Computation},
number = {7},
pages = {1527--1554},
title = {{A fast learning algorithm for deep belief nets}},
volume = {18},
year = {2006}
}
@article{Elman1990,
author = {Elman, J. L.},
doi = {10.1207/s15516709cog1402_1},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive science/1990/Elman - 1990.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive science},
number = {2},
pages = {179--211},
pmid = {19563812},
title = {{Finding structure in time}},
volume = {14},
year = {1990}
}
@incollection{Koutn2008,
address = {Berlin},
author = {Koutnik, Jan and Snorek, Miroslav},
booktitle = {Artificial Neural Networks - ICANN 2008},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Artificial Neural Networks - ICANN 2008/2008/Koutnik, Snorek - 2008.pdf:pdf},
pages = {632--641},
publisher = {Springer},
title = {{Temporal Hebbian Self-Organizing Map for Sequences}},
year = {2008}
}
@article{Varsta2001,
abstract = {This paper compares two Self-Organizing Map (SOM) based$\backslash$nmodels for temporal sequence processing (TSP) both$\backslash$nanalytically and experimentally. These models, Temporal$\backslash$nKohonen Map (TKM) and Recurrent Self-Organizing Map (RSOM),$\backslash$nincorporate leaky integrator memory to preserve the$\backslash$ntemporal context of the input signals. The learning and the$\backslash$nconvergence properties of the TKM and RSOM are studied and$\backslash$nwe show analytically that the RSOM is a significant$\backslash$nimprovement over the TKM, because the RSOM allows simple$\backslash$nderivation of a consistent learning rule. The results of$\backslash$nthe analysis are demonstrated with experiments.},
author = {Varsta, Markus and Heikkonen, Jukka and Lampinen, Jouko and Mill{\'{a}}n, Jos{\'{e}} Del R},
doi = {10.1023/A:1011353011837},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neural Processing Letters/2001/Varsta et al. - 2001.pdf:pdf},
issn = {13704621},
journal = {Neural Processing Letters},
keywords = {Convergence analysis,Self-organizing maps,Temporal sequence processing},
number = {3},
pages = {237--251},
pmid = {21173165},
title = {{Temporal Kohonen map and the recurrent self-organizing map: Analytical and experimental comparison}},
volume = {13},
year = {2001}
}
@article{Mehta2014,
archivePrefix = {arXiv},
arxivId = {1410.3831},
author = {Mehta, Pankaj and Schwab, David J.},
eprint = {1410.3831},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2014/Mehta, Schwab - 2014.pdf:pdf},
title = {{An exact mapping between the Variational Renormalization Group and Deep Learning}},
url = {http://arxiv.org/abs/1410.3831v1},
year = {2014}
}
@article{Voegtlin2002a,
abstract = {This paper explores the combination of self-organizing map (SOM) and feedback, in order to represent sequences of inputs. In general, neural networks with time-delayed feedback represent time implicitly, by combining current inputs and past activities. It has been difficult to apply this approach to SOM, because feedback generates instability during learning. We demonstrate a solution to this problem, based on a nonlinearity. The result is a generalization of SOM that learns to represent sequences recursively. We demonstrate that the resulting representations are adapted to the temporal statistics of the input series. ?? 2002 Elsevier Science Ltd. All rights reserved.},
author = {Voegtlin, Thomas},
doi = {10.1016/S0893-6080(02)00072-2},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neural Networks/2002/Voegtlin - 2002.pdf:pdf},
isbn = {3343791121},
issn = {08936080},
journal = {Neural Networks},
keywords = {Kohonen map,Recurrent networks,Recursive self-organizing maps,Recursiveness,Time},
pages = {979--991},
pmid = {12416688},
title = {{Recursive self-organizing maps}},
volume = {15},
year = {2002}
}
@techreport{Jaeger2014a,
archivePrefix = {arXiv},
arxivId = {arXiv:1403.3369v1},
author = {Jaeger, Herbert},
eprint = {arXiv:1403.3369v1},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2014/Jaeger - 2014(2).pdf:pdf},
institution = {Jacobs University Bremen},
title = {{Controlling Recurrent Neural Networks by Conceptors}},
year = {2014}
}
@article{Deng2013a,
abstract = {This book is aimed to provide an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria: 1) expertise or knowledge of the authors; 2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and 3) the application areas that have the potential to be impacted significantly by deep learning and that have gained concentrated research efforts, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning. In Chapter 1, we provide the background of deep learning, as intrinsically connected to the use of multiple layers of nonlinear transformations to derive features from the sensory signals such as speech and visual images. In the most recent literature, deep learning is embodied also as representation learning, which involves a hierarchy of features or concepts where higher-level representations of them are defined from lower-level ones and where the same lower-level representations help to define higher-level ones. In Chapter 2, a brief historical account of deep learning is presented. In particular, selected chronological development of speech recognition is used to illustrate the recent impact of deep learning that has become a dominant technology in speech recognition industry within only a few years since the start of a collaboration between academic and industrial researchers in applying deep learning to speech recognition. In Chapter 3, a three-way classification scheme for a large body of work in deep learning is developed. We classify a growing number of deep learning techniques into unsupervised, supervised, and hybrid categories, and present qualitative descriptions and a literature survey for each category. From Chapter 4 to Chapter 6, we discuss in detail three popular deep networks and related learning methods, one in each category. Chapter 4 is devoted to deep autoencoders as a prominent example of the unsupervised deep learning techniques. Chapter 5 gives a major example in the hybrid deep network category, which is the discriminative feed-forward neural network for supervised learning with many layers initialized using layer-by-layer generative, unsupervised pre-training. In Chapter 6, deep stacking networks and several of the variants are discussed in detail, which exemplify the discriminative or supervised deep learning techniques in the three-way categorization scheme. In Chapters 7-11, we select a set of typical and successful applications of deep learning in diverse areas of signal and information processing and of applied artificial intelligence. In Chapter 7, we review the applications of deep learning to speech and audio processing, with emphasis on speech recognition organized according to several prominent themes. In Chapters 8, we present recent results of applying deep learning to language modeling and natural language processing. Chapter 9 is devoted to selected applications of deep learning to information retrieval including Web search. In Chapter 10, we cover selected applications of deep learning to image object recognition in computer vision. Selected applications of deep learning to multi-modal processing and multi-task learning are reviewed in Chapter 11. Finally, an epilogue is given in Chapter 12 to summarize what we presented in earlier chapters and to discuss future challenges and directions.},
author = {Deng, Li and Yu, Dong},
doi = {10.1136/bmj.319.7209.0a},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Foundations and Trends in Signal Processing/2013/Deng, Yu - 2013.pdf:pdf},
isbn = {9781405161251},
issn = {09598138},
journal = {Foundations and Trends in Signal Processing},
number = {3-4},
pages = {197----387},
pmid = {10463930},
title = {{Deep Learning: Methods and Applications}},
volume = {7},
year = {2013}
}
@inproceedings{Homenda2014,
author = {Homenda, Wladyslaw and Jastrzebska, Agnieszka and Pedrycz, Witold},
booktitle = {Fourth World Congress on Information and Communication Technologies (WICT), 2014},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Fourth World Congress on Information and Communication Technologies (WICT), 2014/2014/Homenda, Jastrzebska, Pedrycz - 2014.pdf:pdf},
keywords = {-fuzzy cognitive map,cognitive map,fcms has been studied,ii,in,interpretation of fuzzy,l iterature r eview,time series,time series modelling with},
pages = {152--157},
publisher = {IEEE},
title = {{On Interpretation of Fuzzy Cognitive Maps Trained to Model Time Series}},
year = {2014}
}
@book{Bengio2009b,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua},
booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
doi = {10.1561/2200000006},
eprint = {0500581},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Foundations and Trends{\textregistered} in Machine Learning/2009/Bengio - 2009.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
number = {1},
pages = {1--127},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@article{Young2014,
abstract = {Deep machine learning offers a comprehensive framework for extracting meaningful features from complex observations in an unsupervised manner. The majority of deep learning architectures described in the literature primarily focus on extracting spatial features. However, in real-world settings, capturing temporal dependencies in observations is critical for accurate inference. This paper introduces an enhancement to DeSTIN - a compositional deep learning architecture in which each layer consists of multiple instantiations of a common node - that learns to represent spatiotemporal patterns in data based on a novel recurrent clustering algorithm. Contrary to mainstream deep architectures, such as deep belief networks where layer-by-layer training is assumed, each of the nodes in the proposed architecture is trained independently and in parallel. Moreover, top-down and bottom-up information flows facilitate rich feature formation. A semi-supervised setting is demonstrated achieving state-of-the-art results on the MNIST classification benchmarks. A GPU implementation is discussed further accentuating the scalability properties of the proposed framework. ?? 2013 Elsevier Ltd. All rights reserved.},
author = {Young, S. R. and Davis, A. and Mishtal, A. and Arel, I.},
doi = {10.1016/j.patrec.2013.07.013},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Pattern Recognition Letters/2014/Young et al. - 2014.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Deep machine learning,Online clustering,Pattern recognition,Recurrent clustering,Spatiotemporal signals,Unsupervised feature extraction},
number = {1},
pages = {115--123},
publisher = {Elsevier B.V.},
title = {{Hierarchical spatiotemporal feature extraction using recurrent online clustering}},
url = {http://dx.doi.org/10.1016/j.patrec.2013.07.013},
volume = {37},
year = {2014}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Nature/2015/Mnih et al. - 2015.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Miller2006,
abstract = {We present an approach for abstracting invariant classifications of spatiotemporal patterns presented in a highdimensionality input stream, and apply an early proof-of-concept to shift and scale invariant shape recognition. A model called Hierarchical Quilted Self-Organizing Map (HQSOM) is developed, using recurrent self-organizing maps (RSOM) arranged in a pyramidal hierarchy, attempting to mimic the parallel/hierarchical pattern of isocortical processing in the brain. The results of experiments are presented in which the algorithm learns to classify multiple shapes, invariant to shift and scale transformations, in a very small (7×7 pixel) field of view.},
author = {Miller, Jeffrey W and Lommel, Peter H},
doi = {10.1117/12.686183},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the International Society for Optics and Photonics (SPIE)/2006/Miller, Lommel - 2006.pdf:pdf},
isbn = {0819464821},
issn = {0277786X},
journal = {Proceedings of the International Society for Optics and Photonics (SPIE)},
keywords = {Biomimetic,Cognitive Learning,Computational Neuroscience,Image Interpretation,Pattern Recognition,Robot Vision,Self-Organizing Maps (SOM),Unsupervised Learning},
number = {617},
pages = {63840A--63840A--10},
title = {{Biomimetic sensory abstraction using hierarchical quilted self-organizing maps}},
url = {http://link.aip.org/link/PSISDG/v6384/i1/p63840A/s1{\&}Agg=doi},
volume = {6384},
year = {2006}
}
@article{Kohonen1982,
author = {Kohonen, Teuvo},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Biological Cybernetics/1982/Kohonen - 1982.pdf:pdf},
journal = {Biological Cybernetics},
number = {43},
pages = {59--69},
title = {{Self-Organized Formation of Topologically Correct Feature Maps}},
year = {1982}
}
@article{Rauber2002,
author = {Rauber, Andreas and Merkl, Dieter and Dittenbach, Michael},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/IEEE Transactions on Neural Networks/2002/Rauber, Merkl, Dittenbach - 2002.pdf:pdf},
journal = {IEEE Transactions on Neural Networks},
pages = {1331--1341},
title = {{The Growing Hierarchical Self-Organizing Map: Exploratory Analysis of High-Dimensional Data}},
volume = {13},
year = {2002}
}
@article{Guimaraes2002b,
author = {Guimar{\~{a}}es, Gabriela and Lobo, Victor Sousa and Moura-Pires, Fernando},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Intell Data Anal/2002/Guimar{\~{a}}es, Lobo, Moura-Pires - 2002.pdf:pdf},
isbn = {1088-467X},
issn = {1088467X},
journal = {Intell Data Anal},
pages = {1--52},
title = {{A Taxonomy of Self-organizing Maps for Temporal Sequence Processing}},
year = {2002}
}
@article{Tokunaga2009,
abstract = {This study aims to develop a generalized framework of an SOM called a modular network SOM (mnSOM). The mnSOM has an array structure consisting of functional modules that are trainable neural networks, e.g., multi-layer perceptrons (MLPs), instead of the vector units of the conventional SOM. In the case of MLP-modules, an mnSOM learns a group of systems or functions in terms of the input-output relationships in parallel with generating a feature map of them. Thus an mnSOM with MLP modules is an SOM in function space rather than in vector space. In this paper, first, as an example, we focus on a class of mnSOM that consists of MLP modules and introduce the architecture and algorithm. Then, a more generalized framework is described. Finally, some simulation results of an MLP-module-mnSOM are presented. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Tokunaga, Kazuhiro and Furukawa, Tetsuo},
doi = {10.1016/j.neunet.2008.10.006},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neural Networks/2009/Tokunaga, Furukawa - 2009.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Function space,Modular network,SOM},
pages = {82--90},
pmid = {19103475},
title = {{Modular network SOM}},
volume = {22},
year = {2009}
}
@article{Pinto2009a,
abstract = {This paper introduces feedback connections into a previously proposed model of the simple and complex neurons of the neocortex. The original model considers only feedforward connections between a SOM (Self-Organizing Map) and a RSOM (Recurrent SOM). A variant of the SOM-RSOM pair is proposed, called LoopSOM. The RSOM sends predictions to the SOM, providing more robust pattern classification/recognition and solving ambiguities.},
author = {Pinto, R and Engel, P},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the VIII ENIA - Brazilian Meeting on Artificial Intelligence/2009/Pinto, Engel - 2009.pdf:pdf},
journal = {Proceedings of the VIII ENIA - Brazilian Meeting on Artificial Intelligence},
title = {{LoopSOM: A Robust SOM Variant Using Self-Organizing Temporal Feedback Connections}},
year = {2009}
}
@article{Tino2006,
abstract = {Recently there has been an outburst of interest in extending topographic maps of vectorial data to more general data structures, such as sequences or trees. However, there is no general consensus as to how best to process sequences using topographic maps, and this topic remains an active focus of neurocomputational research. The representational capabilities and internal representations of the models are not well understood. Here, we rigorously analyze a generalization of the self-organizing map (SOM) for processing sequential data, recursive SOM(RecSOM) (Voegtlin, 2002), as a nonautonomous dynamical system consisting of a set of fixed input maps. We argue that contractive fixed-input maps are likely to produce Markovian organizations of receptive fields on the RecSOM map. We derive bounds on parameter beta (weighting the importance of importing past information when processing sequences) under which contractiveness of the fixed-input maps is guaranteed. Some generalizations of SOM contain a dynamic module responsible for processing temporal contexts as an integral part of the model. We show that Markovian topographic maps of sequential data can be produced using a simple fixed (nonadaptable) dynamic module externally feeding a standard topographic model designed to process static vectorial data of fixed dimensionality (e.g., SOM). However, by allowing trainable feedback connections, one can obtain Markovian maps with superior memory depth and topography preservation. We elaborate on the importance of non-Markovian organizations in topographic maps of sequential data.},
author = {Tino, Peter and Farkas, Igor and van Mourik, Jort},
doi = {10.1162/neco.2006.18.10.2529},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neural computation/2006/Tino, Farkas, van Mourik - 2006.pdf:pdf},
isbn = {0899-7667 (Print)$\backslash$n0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {10},
pages = {2529--2567},
pmid = {16907636},
title = {{Dynamics and topographic organization of recursive self-organizing maps}},
volume = {18},
year = {2006}
}
@article{VanderVelde2015,
author = {van der Velde, Frank},
doi = {10.1016/j.patrec.2015.02.008},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Pattern Recognition Letters/2015/van der Velde - 2015.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Classification,Classical cognitive science vs. dyn},
pages = {44--52},
publisher = {Elsevier Ltd.},
title = {{Computation and dissipative dynamical systems in neural networks for classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865515000562},
volume = {64},
year = {2015}
}
