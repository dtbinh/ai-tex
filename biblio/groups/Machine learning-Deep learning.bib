Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
eprint = {1312.5602},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/arXiv preprint arXiv {\ldots}/2013/Mnih et al. - 2013.pdf:pdf},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--9},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{VanHasselt2015,
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether this harms performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
archivePrefix = {arXiv},
arxivId = {1509.06461},
author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
eprint = {1509.06461},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/arXiv1509.06461 cs/2015/van Hasselt, Guez, Silver - 2015.pdf:pdf},
journal = {arXiv:1509.06461 [cs]},
title = {{Deep Reinforcement Learning with Double Q-learning}},
url = {http://arxiv.org/abs/1509.06461$\backslash$nhttp://www.arxiv.org/pdf/1509.06461.pdf},
year = {2015}
}
@article{Lillicrap2015,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
doi = {10.1561/2200000006},
eprint = {1509.02971},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2015/Lillicrap et al. - 2015.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
pmid = {24966830},
title = {{Continuous control with deep reinforcement learning}},
url = {http://arxiv.org/abs/1509.02971},
year = {2015}
}
@article{Vinyals2016,
abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6{\%} to 93.2{\%} and from 88.0{\%} to 93.8{\%} on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
archivePrefix = {arXiv},
arxivId = {1606.04080},
author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1606.04080},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/arXiv/2016/Vinyals et al. - 2016.pdf:pdf},
journal = {arXiv},
title = {{Matching Networks for One Shot Learning}},
url = {http://arxiv.org/abs/1606.04080},
year = {2016}
}
@article{Cichy2016,
author = {Cichy, Radoslaw Martin and Khosla, Aditya and Pantazis, Dimitrios and Torralba, Antonio and Oliva, Aude},
doi = {10.1038/srep27755},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Scientific Reports/2016/Cichy et al. - 2016.pdf:pdf},
issn = {2045-2322},
journal = {Scientific Reports},
number = {6},
pmid = {27282108},
title = {{Deep neural networks predict hierarchical spatio-temporal cortical dynamics of human visual object recognition}},
url = {http://dx.doi.org/10.1038/srep27755},
year = {2016}
}
@article{Guo2014,
author = {Guo, Xiaoxiao and Lee, Honglak and Wang, Xiaoshi and Lewis, Richard L},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Nips/2014/Guo et al. - 2014.pdf:pdf},
journal = {Nips},
pages = {1--9},
title = {{Deep learning for real-time Atari game play using offline Monte Carlo tree search planning}},
volume = {2600},
year = {2014}
}
@article{Marblestone2016,
archivePrefix = {arXiv},
arxivId = {1606.03813},
author = {Marblestone, Adam H and Wayne, Greg and Kording, Konrad P},
doi = {10.1101/058545},
eprint = {1606.03813},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2016/Marblestone, Wayne, Kording - 2016.pdf:pdf},
keywords = {cognitive architecture,cost functions,neural networks,neuroscience},
title = {{Towards an integration of deep learning and neuroscience}},
year = {2016}
}
@article{Blundell,
abstract = {State of the art deep reinforcement learning algorithms take many millions of inter-actions to attain human-level performance. Humans, on the other hand, can very quickly exploit highly rewarding nuances of an environment upon first discovery. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Here we investigate whether a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks. We demonstrate that it not only attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains.},
archivePrefix = {arXiv},
arxivId = {1606.04460},
author = {Blundell, Charles and Deepmind, Google and Uria, Benigno and Pritzel, Alexander and Li, Yazhe and Ruderman, Avraham and Leibo, Joel Z and Rae, Jack and Wierstra, Daan and Hassabis, Demis},
eprint = {1606.04460},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/Unknown/Blundell et al. - Unknown.pdf:pdf},
pages = {1--12},
title = {{Model-Free Episodic Control}}
}
@misc{LeCun1989,
abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the US Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
booktitle = {Neural Computation},
doi = {10.1162/neco.1989.1.4.541},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neural Computation/1989/LeCun et al. - 1989.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
number = {4},
pages = {541--551},
pmid = {1000111957},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
volume = {1},
year = {1989}
}
@inproceedings{Novikov2015,
abstract = {Deep neural networks currently demonstrate state-of-the-art performance in sev- eral domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [17] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [21] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06569v1},
author = {Novikov, Alexander and Vetrov, Dmitry and Podoprikhin, Dimitry and Osokin, Anton},
booktitle = {Advances in Neural Information Processing Systems 28 (NIPS 2015)},
eprint = {arXiv:1509.06569v1},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Advances in Neural Information Processing Systems 28 (NIPS 2015)/2015/Novikov et al. - 2015.pdf:pdf},
title = {{Tensorizing Neural Networks}},
url = {http://arxiv.org/pdf/1509.06569v1.pdf},
year = {2015}
}
