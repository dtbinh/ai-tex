Automatically generated by Mendeley Desktop 1.17.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Andrychowicz2016a,
abstract = {In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory. We show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples. In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training. We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.},
archivePrefix = {arXiv},
arxivId = {1602.03218},
author = {Andrychowicz, Marcin and Kurach, Karol},
eprint = {1602.03218},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/International Conference on Machine Learning (ICML)/2016/Andrychowicz, Kurach - 2016.pdf:pdf},
journal = {International Conference on Machine Learning (ICML)},
keywords = {ICML,attention,machine learning},
title = {{Learning Efficient Algorithms with Hierarchical Attentive Memory}},
url = {http://arxiv.org/abs/1602.03218},
year = {2016}
}
