Automatically generated by Mendeley Desktop 1.17.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{MacNeil2011,
abstract = {A central criticism of standard theoretical approaches to constructing stable, recurrent model networks is that the synaptic connection weights need to be finely-tuned. This criticism is severe because proposed rules for learning these weights have been shown to have various limitations to their biological plausibility. Hence it is unlikely that such rules are used to continuously fine-tune the network in vivo. We describe a learning rule that is able to tune synaptic weights in a biologically plausible manner. We demonstrate and test this rule in the context of the oculomotor integrator, showing that only known neural signals are needed to tune the weights. We demonstrate that the rule appropriately accounts for a wide variety of experimental results, and is robust under several kinds of perturbation. Furthermore, we show that the rule is able to achieve stability as good as or better than that provided by the linearly optimal weights often used in recurrent models of the integrator. Finally, we discuss how this rule can be generalized to tune a wide variety of recurrent attractor networks, such as those found in head direction and path integration systems, suggesting that it may be used to tune a wide variety of stable neural systems.},
author = {MacNeil, David and Eliasmith, Chris},
doi = {10.1371/journal.pone.0022885},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/PLoS ONE/2011/MacNeil, Eliasmith - 2011.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pmid = {21980334},
title = {{Fine-tuning and the stability of recurrent neural networks}},
volume = {6},
year = {2011}
}
