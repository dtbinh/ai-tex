Automatically generated by Mendeley Desktop 1.17.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Graves2006,
abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
doi = {10.1145/1143844.1143891},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the 23rd international conference on Machine Learning/2006/Graves et al. - 2006.pdf:pdf},
isbn = {1595933832},
issn = {10987576},
journal = {Proceedings of the 23rd international conference on Machine Learning},
pages = {369--376},
title = {{Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks}},
year = {2006}
}
