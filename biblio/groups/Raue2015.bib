Automatically generated by Mendeley Desktop 1.17.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Raue2015,
abstract = {The problem of how infants learn to associate visual inputs, speech, and internal symbolic representation has long been of interest in Psychology, Neuroscience, and Artificial Intelligence. A priori, both visual inputs and auditory inputs are complex analog signals with a large amount of noise and context, and lacking of any segmentation information. In this paper, we address a simple form of this problem: the association of one visual input and one auditory input with each other. We show that the presented model learns both segmentation, recognition and symbolic representation under two simple assumptions: (1) that a symbolic representation exists, and (2) that two different inputs represent the same symbolic structure. Our approach uses two Long Short-Term Memory (LSTM) networks for multimodal sequence learning and recovers the internal symbolic space using an EM-style algorithm. We compared our model against LSTM in three different multimodal datasets: digit, letter and word recognition. The performance of our model reached similar results to LSTM.},
author = {Raue, Federico and Breuel, Thomas M. and Liwicki, Marcus},
booktitle = {Proceedings of the NIPS Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches},
editor = {Besold, Tarek R. and d'Avila Garcez, Artur and Marcus, Gary F. and Miikkulainen, Risto},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the NIPS Workshop on Cognitive Computation Integrating Neural and Symbolic Approaches/2015/Raue, Breuel, Liwicki - 2015.pdf:pdf},
pages = {1--9},
title = {{Symbol Grounding in Multimodal Sequences using Recurrent Neural Networks}},
year = {2015}
}
