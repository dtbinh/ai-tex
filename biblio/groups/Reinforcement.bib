Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Fu2006,
abstract = {The authors propose a reinforcement-learning mechanism as a model for recurrent choice and extend it to account for skill learning. The model was inspired by recent research in neurophysiological studies of the basal ganglia and provides an integrated explanation of recurrent choice behavior and skill learning. The behavior includes effects of differential probabilities, magnitudes, variabilities, and delay of reinforcement. The model can also produce the violation of independence, preference reversals, and the goal gradient of reinforcement in maze learning. An experiment was conducted to study learning of action sequences in a multistep task. The fit of the model to the data demonstrated its ability to account for complex skill learning. The advantages of incorporating the mechanism into a larger cognitive architecture are discussed.},
author = {Fu, Wai-Tat and Anderson, John R},
doi = {10.1037/0096-3445.135.2.184},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Journal of experimental psychology. General/2006/Fu, Anderson - 2006.pdf:pdf},
isbn = {0096-3445$\backslash$n1939-2222},
issn = {0096-3445},
journal = {Journal of experimental psychology. General},
keywords = {Animals,Basal Ganglia,Basal Ganglia: physiology,Choice Behavior,Cognition,Cognition: physiology,Humans,Maze Learning,Models, Psychological,Probability,Rats,Reinforcement (Psychology),Reinforcement Schedule,Serial Learning},
number = {2},
pages = {184--206},
pmid = {16719650},
title = {{From recurrent choice to skill learning: a reinforcement-learning model.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16719650},
volume = {135},
year = {2006}
}
@incollection{Xia2015,
address = {Paris},
author = {Xia, Chen and {El Kamel}, A.},
booktitle = {Proceedings of the 21st International Conference on Industrial Engineering and Engineering Management 2014},
doi = {10.2991/978-94-6239-102-4_136},
editor = {Qi, Ershi and Shen, Jiang and Dou, Runliang},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proceedings of the 21st International Conference on Industrial Engineering and Engineering Management 2014/2015/Xia, El Kamel - 2015.pdf:pdf},
isbn = {978-94-6239-101-7},
keywords = {automation,capacity building,f rxqghu,ngo,npo,resource mismanagement,syndrome,v},
pages = {671--675},
publisher = {Atlantis Press},
series = {Proceedings of the International Conference on Industrial Engineering and Engineering Management},
title = {{A Reinforcement Learning Method of Obstacle Avoidance for Industrial Mobile Vehicles in Unknown Environments Using Neural Network}},
url = {http://link.springer.com/10.2991/978-94-6239-102-4 http://link.springer.com/10.2991/978-94-6239-102-4{\_}136},
volume = {2014},
year = {2015}
}
@article{Zhang2017,
author = {Zhang, Fengyun and Duan, Shukai and Wang, Lidan},
doi = {10.1007/s11571-017-9423-7},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Neurodynamics/2017/Zhang, Duan, Wang - 2017.pdf:pdf},
issn = {1871-4080},
journal = {Cognitive Neurodynamics},
keywords = {Route searching,Neural network,Heuristic reinforce,exploitation,network {\'{a}} heuristic,reinforcement learning {\'{a}} greedy,route searching {\'{a}} neural},
publisher = {Springer Netherlands},
title = {{Route searching based on neural networks and heuristic reinforcement learning}},
url = {http://link.springer.com/10.1007/s11571-017-9423-7},
year = {2017}
}
@inproceedings{Sharma2017,
author = {Sharma, Avinash and Gupta, Kanika and Kumar, Anirudha and Sharma, Aishwarya and Kumar, Rajesh and Member, Senior},
booktitle = {2017 IEEE International Conference on Industrial Technology (ICIT)},
doi = {10.1109/ICIT.2017.7915468},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/2017 IEEE International Conference on Industrial Technology (ICIT)/2017/Sharma et al. - 2017.pdf:pdf},
keywords = {Control System,Robotics and Automation},
pages = {837--842},
publisher = {IEEE},
title = {{Model based path planning using Q-Learning}},
year = {2017}
}
@article{Tesauro1995,
author = {Tesauro, G.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Communications of the ACM/1995/Tesauro - 1995.pdf:pdf},
journal = {Communications of the ACM},
number = {5},
pages = {58--68},
title = {{Temporal Difference Learning and TD-Gammon}},
volume = {38},
year = {1995}
}
@article{Sun2005,
abstract = {This article explicates the interaction between implicit and explicit processes in skill learning, in contrast to the tendency of researchers to study each type in isolation. It highlights various effects of the interaction on learning (including synergy effects). The authors argue for an integrated model of skill learning that takes into account both implicit and explicit processes. Moreover, they argue for a bottom-up approach (first learning implicit knowledge and then explicit knowledge) in the integrated model. A variety of qualitative data can be accounted for by the approach. A computational model, CLARION, is then used to simulate a range of quantitative data. The results demonstrate the plausibility of the model, which provides a new perspective on skill learning.},
author = {Sun, Ron and Slusarz, Paul and Terry, Chris},
doi = {10.1037/0033-295X.112.1.159},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Psychological Review/2005/Sun, Slusarz, Terry - 2005.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471},
journal = {Psychological Review},
keywords = {artificial grammars,connectionist,decision-making,information,memory,model,nonconscious acquisition,procedural knowledge,tacit knowledge,task-performance},
number = {1},
pages = {159--192},
pmid = {15631592},
title = {{The Interaction of the Explicit and the Implicit in Skill Learning: A Dual-Process Approach.}},
volume = {112},
year = {2005}
}
@incollection{Ponsen2010,
abstract = {In this paper we survey the basics of reinforcement learning, gener- alization and abstraction. We start with an introduction to the fundamentals of reinforcement learning and motivate the necessity for generalization and abstrac- tion. Next we summarize themost important techniques available to achieve both generalization and abstraction in reinforcement learning. We discuss basic func- tion approximation techniques and delve into hierarchical, relational and transfer learning. All concepts and techniques are illustrated with examples.},
author = {Ponsen, Marc and Taylor, Matthew E. and Tuyls, Karl},
booktitle = {Adaptive and Learning Agents},
doi = {10.1007/978-3-642-11814-2_1},
editor = {Taylor, Matthew E. and Tuyls, Karl},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Adaptive and Learning Agents/2010/Ponsen, Taylor, Tuyls - 2010.pdf:pdf},
isbn = {3642118135},
issn = {03029743},
pages = {1--32},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Abstraction and generalization in reinforcement learning: A summary and framework}},
year = {2010}
}
@article{Melo1997,
abstract = {We address the problem of computing the optimal Q-function in Markov decision problems with infinite state-space. We analyze the convergence properties of several variations of Q-learning when combined with function approximation, extending the analysis of TD-learning in (Tsitsiklis {\&} Van Roy, 1996a) to stochastic control settings. We identify conditions under which such approximate methods converge with probability 1. We conclude with a brief discussion on the general applicability of our results and compare them with several related works.},
author = {Melo, F S and Meyn, S P and Ribeiro, M I},
doi = {10.1109/9.580874},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/IEEE Transactions on Automatic Control/1997/Melo, Meyn, Ribeiro - 1997.pdf:pdf},
isbn = {9781605582054},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
keywords = {planning reinforcement learning Q-learning tempora},
number = {5},
pages = {674--690},
title = {{An analysis of reinforcement learning with function approximation}},
volume = {42},
year = {1997}
}
@book{Sutton2012,
address = {London},
author = {Sutton, Richard S. and Barto, Andrew G.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2012/Sutton, Barto - 2012.pdf:pdf},
pages = {320},
publisher = {The MIT Press},
title = {{Reinforcement learning: An Introduction}},
url = {https://books.google.com/books?id=CAFR6IBF4xYC{\&}pgis=1{\%}5Cnhttp://incompleteideas.net/sutton/book/the-book.html{\%}5Cnhttps://www.dropbox.com/s/f4tnuhipchpkgoj/book2012.pdf},
year = {2012}
}
@article{Li2013,
abstract = {Pulse Coupled Neural Network (PCNN) is suitable for dealing with the classical shortest path problem, because of its autowave characteristic. However, most methods suggest that the autowave of PCNN models should keep a constant speed in finding the shortest paths. This paper proposes a novel self-adaptive autowave pulse-coupled neural network (SAPCNN) model for the shortest path problem. The autowave generated by SAPCNN propagates adaptively according to the current network state, which guarantees it spreads more effectively in finding the shortest paths. Our experiments, which have been carried out for both the shortest paths problem and K shortest paths problem, show that our proposed algorithm outperforms classical algorithms. ?? 2013 Elsevier B.V.},
author = {Li, Xiaojun and Ma, Yide and Feng, Xiaowen},
doi = {10.1016/j.neucom.2012.12.030},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neurocomputing/2013/Li, Ma, Feng - 2013.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Autowave,Pulse-coupled neural network,Self-adaptive autowave PCNN,Shortest path},
pages = {63--71},
publisher = {Elsevier},
title = {{Self-adaptive autowave pulse-coupled neural network for shortest-path problem}},
url = {http://dx.doi.org/10.1016/j.neucom.2012.12.030},
volume = {115},
year = {2013}
}
@inproceedings{Dickens2010,
author = {Dickens, Luke and Broda, Krysia and Russo, Alessandra},
booktitle = {ECAI 2010: 19th European Conference on Artificial Intelligence, 16-20 August 2010, Lisbon, Portugal : Including Prestigious Applications of Artificial Intelligence (PAIS-2010) : Proceedings},
doi = {10.3233/978-1-60750-606-5-367},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/ECAI 2010 19th European Conference on Artificial Intelligence, 16-20 August 2010, Lisbon, Portugal Including Prestigious Applications of Ar./2010/Dickens, Broda, Russo - 2010.pdf:pdf},
isbn = {9781607506065},
pages = {367--372},
title = {{The Dynamics of Multi-Agent Reinforcement Learning}},
year = {2010}
}
@article{Duan2016,
abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophis-ticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, bene-fiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a " fast " reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL 2 , the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (" slow ") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including ob-servations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the " fast " RL algorithm on the current (previously unseen) MDP. We evaluate RL 2 experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL 2 is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL 2 on a vision-based navigation task and show that it scales up to high-dimensional problems.},
archivePrefix = {arXiv},
arxivId = {1611.02779},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter and Sutskever, Ilya and Abbeel, Pieter},
doi = {10.1051/0004-6361/201527329},
eprint = {1611.02779},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/arXiv/2016/Duan et al. - 2016.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv},
pages = {1--14},
title = {{RL{\^{}}2: Fast Reinforcement Learning Via Slow Reinforcement Learning}},
year = {2016}
}
@inproceedings{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
booktitle = {arXiv: 1312.5602},
eprint = {1312.5602},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/arXiv 1312.5602/2013/Mnih et al. - 2013.pdf:pdf},
pages = {1--9},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Shankar2017,
abstract = {Deep Reinforcement Learning has enabled the learning of policies for complex tasks in partially observable environments, without explicitly learning the underlying model of the tasks. While such model-free methods achieve considerable performance, they often ignore the structure of task. We present a natural representation of to Reinforcement Learning (RL) problems using Recurrent Convolutional Neural Networks (RCNNs), to better exploit this inherent structure. We define 3 such RCNNs, whose forward passes execute an efficient Value Iteration, propagate beliefs of state in partially observable environments, and choose optimal actions respectively. Backpropagating gradients through these RCNNs allows the system to explicitly learn the Transition Model and Reward Function associated with the underlying MDP, serving as an elegant alternative to classical model-based RL. We evaluate the proposed algorithms in simulation, considering a robot planning problem. We demonstrate the capability of our framework to reduce the cost of replanning, learn accurate MDP models, and finally re-plan with learnt models to achieve near-optimal policies.},
archivePrefix = {arXiv},
arxivId = {1701.02392},
author = {Shankar, Tanmay and Dwivedy, Santosha K. and Guha, Prithwijit},
doi = {10.1109/ICPR.2016.7900026},
eprint = {1701.02392},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/ArXiv 1701.02392/2017/Shankar, Dwivedy, Guha - 2017.pdf:pdf},
isbn = {9781509048472},
issn = {10514651},
journal = {ArXiv: 1701.02392},
title = {{Reinforcement Learning via Recurrent Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1701.02392},
year = {2017}
}
@article{Levine2016,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1504.00702},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Journal of Machine Learning Research/2016/Levine et al. - 2016.pdf:pdf},
isbn = {9781479969227},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Neural Networks,Optimal Control,Reinforcement Learning,Vision},
pages = {1--40},
pmid = {15003161},
title = {{End-to-End Training of Deep Visuomotor Policies}},
volume = {17},
year = {2016}
}
@article{Wang2016,
abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
archivePrefix = {arXiv},
arxivId = {1611.05763},
author = {Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
eprint = {1611.05763},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2016/Wang et al. - 2016.pdf:pdf},
month = {nov},
pages = {1--17},
title = {{Learning to reinforcement learn}},
url = {http://arxiv.org/abs/1611.05763},
year = {2016}
}
@misc{Brockam2016,
abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
booktitle = {ArXiv: 1606.01540},
eprint = {1606.01540},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/ArXiv 1606.01540/2016/Brockman et al. - 2016.pdf:pdf},
pages = {1--4},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@phdthesis{Lin1993,
author = {Lin, Long-ji},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/1993/Lin - 1993.pdf:pdf},
pages = {1--155},
school = {CMU},
title = {{Reinforcement Learning for Robots Using Neural Networks}},
year = {1993}
}
@incollection{Riedmiller2005,
abstract = {This paper introduces NFQ, an algorithm for efficient and ef- fective training of a Q-value function represented by amulti-layer percep- tron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
author = {Riedmiller, Martin},
booktitle = {Machine Learning: ECML 2005},
doi = {10.1007/11564096_32},
editor = {Gama, Jo{\~{a}}o and Camacho, Rui and Brazdil, Pavel B. and Jorge, Al{\'{i}}pio M{\'{a}}rio and Torgo, Lu{\'{i}}s},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Machine Learning ECML 2005/2005/Riedmiller - 2005.pdf:pdf},
isbn = {3540292438},
issn = {03029743},
pages = {317--328},
publisher = {Springer Berlin Heidelberg},
title = {{Neural Fitted Q Iteration – First Experiences with a Data Efficient Neural Reinforcement Learning Method}},
url = {http://link.springer.com/10.1007/11564096{\_}32},
year = {2005}
}
@inproceedings{Johnson2016,
abstract = {We present Project Malmo – an AI experimenta-tion platform built on top of the popular computer game Minecraft, and designed to support funda-mental research in artificial intelligence. As the AI research community pushes for artificial gen-eral intelligence (AGI), experimentation platforms are needed that support the development of flexible agents that learn to solve diverse tasks in complex environments. Minecraft is an ideal foundation for such a platform, as it exposes agents to complex 3D worlds, coupled with infinitely varied game-play. Project Malmo provides a sophisticated abstraction layer on top of Minecraft that supports a wide range of experimentation scenarios, ranging from navi-gation and survival to collaboration and problem solving tasks. In this demo we present the Malmo platform and its capabilities. The platform is pub-licly released as open source software at IJCAI, to support openness and collaboration in AI research.},
author = {Johnson, Matthew and Hofmann, Katja and Hutton, Tim and Bignell, David},
booktitle = {Proc. 25th International Joint Conference on Artificial Intelligence},
editor = {Kambhampati, S.},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Proc. 25th International Joint Conference on Artificial Intelligence/2016/Johnson et al. - 2016.pdf:pdf},
issn = {10450823},
keywords = {Demonstrations},
pages = {4246--4247},
publisher = {AAAI Press},
title = {{The Malmo Platform for Artificial Intelligence Experimentation}},
year = {2016}
}
@article{Li2017,
abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We start with background of deep learning and reinforcement learning, as well as introduction of testbeds. Next we discuss Deep Q-Network (DQN) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised learning, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, spoken dialogue systems (a.k.a. chatbot), machine translation, text sequence prediction, neural architecture design, personalized web services, healthcare, finance, and music generation. We mention topics/papers not reviewed yet. After listing a collection of RL resources, we close with discussions.},
archivePrefix = {arXiv},
arxivId = {1701.07274},
author = {Li, Yuxi},
eprint = {1701.07274},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2017/Li - 2017.pdf:pdf},
pages = {1--30},
title = {{Deep Reinforcement Learning: An Overview}},
url = {http://arxiv.org/abs/1701.07274},
year = {2017}
}
@article{Sutton1999,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Artificial Intelligence/1999/Sutton, Precup, Singh - 1999(2).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Artificial Intelligence},
pages = {181--211},
pmid = {25246403},
title = {{Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning}},
volume = {112},
year = {1999}
}
@article{Liu2015,
abstract = {The K Shortest Paths (KSPs) problem with non-numerable applications has been researched widely, which aims to compute KSPs between two nodes in a non-decreasing order. However, less effort has been devoted to single-source KSP problem than to single-pair KSP computation, especially by using parallel methods. This paper proposes a Modified Continued Pulse Coupled Neural Network (MCPCNN) model to solve the two kinds of KSP problems. Theoretical analysis of MCPCNN and two algorithms for KSPs computation are presented. By using the parallel pulse transmission characteristic of pulse coupled neural networks, the method is able to find k shortest paths quickly. The computational complexity is only related to the length of the longest shortest path. Simulative results for route planning show that the proposed MCPCNN method for KSPs computation outperforms many other current efficient algorithms.},
author = {Liu, Guisong and Qiu, Zhao and Qu, Hong and Ji, Luping},
doi = {10.1016/j.neucom.2014.09.012},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Neurocomputing/2015/Liu et al. - 2015.pdf:pdf},
isbn = {9780874216561},
issn = {18728286},
journal = {Neurocomputing},
keywords = {K Shortest paths,Pulse coupled neural network,Single-pair KSP,Single-source KSP},
number = {PC},
pages = {1162--1176},
publisher = {Elsevier},
title = {{Computing k shortest paths using modified pulse-coupled neural network}},
url = {http://dx.doi.org/10.1016/j.neucom.2014.09.012},
volume = {149},
year = {2015}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Nature/2015/Mnih et al. - 2015.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@book{Sun2001,
abstract = {This paper presents a skill learning model Clarion. Different from existing models of mostly high-level skill learning that use a top-down approach (that is, turning declarative knowledge into procedural knowledge through practice), we adopt a bottom-up approach toward low-level skill learning, where procedural knowledge develops first and declarative knowledge develops later. Our model is formed by integrating connectionist, reinforcement, and symbolic learning methods to perform on-line reactive learning. It adopts a two-level dual-representation framework (Sun, 1995), with a combination of localist and distributed representation. We compare the model with human data in a minefield navigation task, demonstrating some match between the model and human data in several respects. ?? 2001 Cognitive Science Society, Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sun, Ron and Merrill, Edward and Peterson, Todd},
booktitle = {Cognitive Science},
doi = {10.1016/S0364-0213(01)00035-0},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Cognitive Science/2001/Sun, Merrill, Peterson - 2001.pdf:pdf},
isbn = {1573884766},
issn = {03640213},
number = {2},
pages = {203--244},
pmid = {15003161},
title = {{From implicit skills to explicit knowledge: A bottom-up model of skill learning}},
volume = {25},
year = {2001}
}
@article{Yang2004,
author = {Yang, S.X. and Luo, Chaomin},
doi = {10.1109/TSMCB.2003.811769},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)/2004/Yang, Luo - 2004.pdf:pdf},
issn = {1083-4419},
journal = {IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)},
month = {feb},
number = {1},
pages = {718--724},
title = {{A Neural Network Approach to Complete Coverage Path Planning}},
url = {http://ieeexplore.ieee.org/document/1262545/},
volume = {34},
year = {2004}
}
@article{Qu2009,
author = {Qu, Hong and Yang, Simon X and Willms, Allan R and Yi, Zhang},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/IEEE Transactions on Neural Networks/2009/Qu et al. - 2009.pdf:pdf},
isbn = {1045-9227},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
number = {11},
pages = {1724--1739},
title = {{Real-time robot path planning based on a modified pulse-coupled neural network model}},
volume = {20},
year = {2009}
}
@inproceedings{Lange2010,
author = {Lange, Sascha and Riedmiller, Martin},
booktitle = {The 2010 International Joint Conference on Neural Networks (IJCNN)},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/The 2010 International Joint Conference on Neural Networks (IJCNN)/2010/Lange, Riedmiller - 2010.pdf:pdf},
pages = {1--8},
publisher = {IEEE},
title = {{Deep Auto-Encoder Neural Networks in Reinforcement Learning}},
year = {2010}
}
@book{Sutton2011,
address = {М.},
author = {Саттон, Р.С. and Барто, Э. Г.},
edition = {2-е},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Unknown/2011/Саттон, Барто - 2011.pdf:pdf},
language = {russian},
pages = {399},
publisher = {БИНОМ. Лаборатория знаний},
title = {{Обучение с подкреплением}},
year = {2011}
}
@article{Xia2016,
abstract = {Designing intelligent and robust autonomous navigation systems remains a great challenge in mobile robotics. Inverse reinforcement learning (IRL) offers an efficient learning technique from expert demonstrations to teach robots how to perform specific tasks without manually specifying the reward function. Most of existing IRL algorithms assume the expert policy to be optimal and deterministic, and are applied to experiments with relatively small-size state spaces. However, in autonomous navigation tasks, the state spaces are frequently large and demonstrations can hardly visit all the states. Meanwhile the expert policy may be non-optimal and stochastic. In this paper, we focus on IRL with large-scale and high-dimensional state spaces by introducing the neural network to generalize the expert's behaviors to unvisited regions of the state space and an explicit policy representation is easily expressed by neural network, even for the stochastic expert policy. An efficient and convenient algorithm, Neural Inverse Reinforcement Learning (NIRL), is proposed. Experimental results on simulated autonomous navigation tasks show that a mobile robot using our approach can successfully navigate to the target position without colliding with unpredicted obstacles, largely reduce the learning time, and has a good generalization performance on undemonstrated states. Hence prove the robot intelligence of autonomous navigation transplanted from limited demonstrations to completely unknown tasks.},
author = {Xia, Chen and {El Kamel}, Abdelkader},
doi = {10.1016/j.robot.2016.06.003},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Robotics and Autonomous Systems/2016/Xia, El Kamel - 2016.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous navigation,Dynamic environments,Inverse reinforcement learning,Learning from demonstration,Markov decision processes,Neural network},
pages = {1--14},
publisher = {Elsevier B.V.},
title = {{Neural inverse reinforcement learning in autonomous navigation}},
url = {http://dx.doi.org/10.1016/j.robot.2016.06.003},
volume = {84},
year = {2016}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task involving finding rewards in random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/arXiv/2016/Mnih et al. - 2016.pdf:pdf},
isbn = {9781510829008},
journal = {arXiv},
pages = {1--28},
pmid = {1000272564},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@article{Kaelbling1996,
abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
doi = {10.1613/jair.301},
eprint = {9605103},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Journal of Artificial Intelligence Research/1996/Kaelbling, Littman, Moore - 1996.pdf:pdf},
isbn = {0-7803-3213-X},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {237--285},
pmid = {17255001},
primaryClass = {cs},
title = {{Reinforcement learning: A survey}},
volume = {4},
year = {1996}
}
@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q,-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
archivePrefix = {arXiv},
arxivId = {1412.3409},
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
doi = {10.1023/A:1022676722315},
eprint = {1412.3409},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Machine Learning/1992/Watkins, Dayan - 1992.pdf:pdf},
isbn = {978-1-4613-6608-9},
issn = {15730565},
journal = {Machine Learning},
keywords = {(Formula presented.)-learning,asynchronous dynamic programming,reinforcement learning,temporal differences},
number = {3},
pages = {279--292},
pmid = {7761831},
title = {{Q-Learning}},
volume = {8},
year = {1992}
}
@article{YangLuShujuanYiYurongLiu2016,
author = {Lu, Yang and Yi, Shujuan and Liu, Yurong and Ji, Yuling},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/Assembly Automation/2016/Lu et al. - 2016.pdf:pdf},
journal = {Assembly Automation},
number = {2},
title = {{A Novel Path Planning Method for Biomimetic Robot Based on Deep Learning}},
volume = {36},
year = {2016}
}
@article{Paxton2017,
abstract = {We consider task and motion planning in complex dynamic environments for problems expressed in terms of a set of Linear Temporal Logic (LTL) constraints, and a reward function. We propose a methodology based on reinforcement learning that employs deep neural networks to learn low-level control policies as well as task-level option policies. A major challenge in this setting, both for neural network approaches and classical planning, is the need to explore future worlds of a complex and interactive environment. To this end, we integrate Monte Carlo Tree Search with hierarchical neural net control policies trained on expressive LTL specifications. This paper investigates the ability of neural networks to learn both LTL constraints and control policies in order to generate task plans in complex environments. We demonstrate our approach in a simulated autonomous driving setting, where a vehicle must drive down a road in traffic, avoid collisions, and navigate an intersection, all while obeying given rules of the road.},
archivePrefix = {arXiv},
arxivId = {1703.07887},
author = {Paxton, Chris and Raman, Vasumathi and Hager, Gregory D. and Kobilarov, Marin},
eprint = {1703.07887},
file = {:C$\backslash$:/Users/sanek/Documents/Mendeley Desktop/ArXiv 1703.07887/2017/Paxton et al. - 2017.pdf:pdf},
journal = {ArXiv: 1703.07887},
title = {{Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments}},
url = {http://arxiv.org/abs/1703.07887},
year = {2017}
}
